[
  {
    "objectID": "eco309/index_2026.html",
    "href": "eco309/index_2026.html",
    "title": "econobits",
    "section": "",
    "text": "Work in Progress"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "econobits",
    "section": "",
    "text": "Ongoing courses:\nMIE37\nECO309\nSciencesPo"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#introduction",
    "href": "lectures/discrete_dynamic_programming/index.html#introduction",
    "title": "Discrete Dynamic Programming",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#section",
    "href": "lectures/discrete_dynamic_programming/index.html#section",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "The imperialism of Dynamic Programming\n— Recursive Macroeconomic Theory (Ljunqvist & Sargent)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#section-1",
    "href": "lectures/discrete_dynamic_programming/index.html#section-1",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "I spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. An interesting question is, “Where did the name, dynamic programming, come from?” The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word “research”. I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term research in his presence. You can imagine how he felt, then, about the term mathematical. The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word “programming”. I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let’s kill two birds with one stone. Let’s take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it’s impossible to use the word dynamic in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It’s impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.\n\n— Richard Bellman, Eye of the Hurricane: An Autobiography (1984, page 159)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#markov-chain-and-markov-process-1",
    "href": "lectures/discrete_dynamic_programming/index.html#markov-chain-and-markov-process-1",
    "title": "Discrete Dynamic Programming",
    "section": "Markov chain and Markov process",
    "text": "Markov chain and Markov process\n\nStochastic process: family of random variables indexed by time\nA stochastic process has the Markov property if its future evolution depends only on its current state.\nSpecial cases:\n\n\n\n\n\n\n\n\n\n\nDiscrete States\nContinuous States\n\n\n\n\nDiscrete Time\nDiscrete Markov Chain\nContinuous Markov Chain\n\n\nContinuous Time\nMarkov Jump Process\nMarkov Process"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#stochastic-matrices",
    "href": "lectures/discrete_dynamic_programming/index.html#stochastic-matrices",
    "title": "Discrete Dynamic Programming",
    "section": "Stochastic matrices",
    "text": "Stochastic matrices\n\na matrix \\(M \\in R^n\\times R^n\\) matrix is said to be stochastic if\n\nall coefficents are non-negative\nall the lines lines sum to 1 (\\(\\forall i, \\sum_j M_{ij} = 1\\))\n\na probability density is a vector \\(\\mu \\in R^n\\) such that :\n\nall components are non-negative\nall coefficients sum to 1 (\\(\\sum_{i=1}^n \\mu_{i} = 1\\))\n\na distribution is a vector with such that:\n\nall components are non-negative"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#simulation",
    "href": "lectures/discrete_dynamic_programming/index.html#simulation",
    "title": "Discrete Dynamic Programming",
    "section": "Simulation",
    "text": "Simulation\n\nConsider: \\(\\mu_{i,t+1}' =\\mu_t' P\\)\nWe have \\(\\mu_{i,t+1} = \\sum_{k=1}^n  \\mu_{k,t}  P_{k, i}\\)\nAnd: \\(\\sum_i\\mu_{i,t+1} = \\sum_i \\mu_{i,t}\\)\nPostmultiplication by a stochastic matrix preserves the mass.\nInterpretation: \\(P_{ij}\\) is the fraction of the mass initially in state \\(i\\) which ends up in \\(j\\)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#example",
    "href": "lectures/discrete_dynamic_programming/index.html#example",
    "title": "Discrete Dynamic Programming",
    "section": "Example",
    "text": "Example\n\\[\\underbrace{\n\\begin{pmatrix}\n? & ? & ?\n\\end{pmatrix}\n}_{\\mu_{t+1}'} = \\underbrace{\n\\begin{pmatrix}\n0.5 & 0.3 & 0.2\n\\end{pmatrix}\n}_{\\mu_t'} \\begin{pmatrix}\n0.4 & 0.6 & 0.0 \\\\\\\\\n0.2 & 0.5 & 0.3 \\\\\\\\\n0 & 0 & 1.0\n\\end{pmatrix}\\]\n\nGraphical Representation:"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#probabilistic-interpretation",
    "href": "lectures/discrete_dynamic_programming/index.html#probabilistic-interpretation",
    "title": "Discrete Dynamic Programming",
    "section": "Probabilistic interpretation",
    "text": "Probabilistic interpretation\n\nDenote by \\(S=(s_1,...s_n)\\) a finite set with \\(n\\) elements (\\(|S|=n\\)).\nA Markov Chain with values in \\(S\\) and with transitions given by a stochastic matrix \\(P\\in R^n\\times R^n\\) identfies a stochastic process \\((X_t)_{t\\geq 0}\\) such that \\[P_{ij} = Prob(X_{t+1}=s_j|X_t=s_i)\\]\nIn words, line \\(i\\) describes the conditional distribution of \\(X_{t+1}\\) conditional on \\(X_t=s_i\\)."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#what-about-longer-horizons",
    "href": "lectures/discrete_dynamic_programming/index.html#what-about-longer-horizons",
    "title": "Discrete Dynamic Programming",
    "section": "What about longer horizons?",
    "text": "What about longer horizons?\n\nIt is easy to show that for any \\(k\\), \\(P^k\\) is a stochastic matrix.\n\\(P^k_{ij}\\) denotes the probability of ending in \\(j\\), after \\(k\\) periods, starting from \\(i\\)\nGiven an initial distribution \\(\\mu_0\\in R^{+ n}\\)\n\nWhich states will visited with positive probability between t=0 and t=k?\nWhat happens in the very long run?\n\nWe need to study a little bit the properties of Markov Chains"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#connectivity",
    "href": "lectures/discrete_dynamic_programming/index.html#connectivity",
    "title": "Discrete Dynamic Programming",
    "section": "Connectivity",
    "text": "Connectivity\n\nTwo states \\(s_i\\) and \\(s_j\\) are connected if \\(P_{ij}&gt;0\\)\nWe call incidence matrix: \\(\\mathcal{I}(P)=(\\delta_{P_{ij}&gt;0})_{ij}\\)\nTwo states \\(i\\) and \\(j\\) communicate with each other if there are \\(k\\) and \\(l\\) such that: \\((P^k)_ {i,j}&gt;0\\) and \\((P^l)_ {j,i}&gt;0\\)\n\nit is an equivalence relation\nwe can define equivalence classes\n\nA stochastic matrix \\(P\\) is irreducible if all states communicate\n\nthere is a unique communication class"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#connectivity-and-irreducibility",
    "href": "lectures/discrete_dynamic_programming/index.html#connectivity-and-irreducibility",
    "title": "Discrete Dynamic Programming",
    "section": "Connectivity and irreducibility",
    "text": "Connectivity and irreducibility\n\n\nIrreducible\n\n\n\n\n\n\n\n\n\n\n\nIrreducible: all states can be reached with positive probability from any initial state.\n\nNot irreducible\n\n\n\n\n\n\n\n\n\n\n\n\nHere there is a subset of states (poor), which absorbs all the mass coming in."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#aperiodicity",
    "href": "lectures/discrete_dynamic_programming/index.html#aperiodicity",
    "title": "Discrete Dynamic Programming",
    "section": "Aperiodicity",
    "text": "Aperiodicity\n\nAre there cycles? Starting from a state \\(i\\), how long does it take to return to \\(i\\)?\nThe period of a state is defined as \\[gcd( {k\\geq 1 | (P^k)_{i,i}&gt;0} )\\]\nIf a state has a period d&gt;1 the chain returns to the state only at dates multiple of d."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#aperiodicity-1",
    "href": "lectures/discrete_dynamic_programming/index.html#aperiodicity-1",
    "title": "Discrete Dynamic Programming",
    "section": "Aperiodicity",
    "text": "Aperiodicity\n\n\nPeriodic\n\n\n\n\n\n\n\n\n\n\n\n\nIf you start from some states, you return to it, but not before two periods.\n\n\nAperiodic\n\n\n\n\n\n\n\n\n\n\n\n\nIf some mass leaves a state, some of it returns to the state in the next period."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#stationary-distribution",
    "href": "lectures/discrete_dynamic_programming/index.html#stationary-distribution",
    "title": "Discrete Dynamic Programming",
    "section": "Stationary distribution",
    "text": "Stationary distribution\n\n\\(\\mu\\) is a stationary distribution if \\(\\mu' = \\mu' P\\)\nTheorem: there always exists such a distribution\n\nproof: Brouwer theorem (fixed-point result for compact-convex set)\n\\(f: \\mu\\rightarrow (\\mu'P)'\\)\n\nTheorem:\n\nif P is irreducible the fixed point \\(\\mu^{\\star}\\) is unique\nif P is irreducible and aperiodic \\(|\\mu_0' P^k - \\mu^{\\star}| \\underset{k\\to+\\infty}{\\longrightarrow}0\\) for any initial distribution \\(\\mu_0\\)\n\nWe then say the Markov chain is ergodic\n\\(\\mu^{\\star}\\) is the ergodic distribution\n\nit is the best guess, one can do for the state of the chain in the very far future"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#stationary-distribution-proof",
    "href": "lectures/discrete_dynamic_programming/index.html#stationary-distribution-proof",
    "title": "Discrete Dynamic Programming",
    "section": "Stationary distribution (proof)",
    "text": "Stationary distribution (proof)\n\nBrouwer’s theorem: Let \\(\\mathcal{C}\\) be a compact convex subset of \\(R^n\\) and \\(f\\) a continuous mapping \\(\\mathcal{C}\\rightarrow \\mathcal{C}\\). Then there exists a fixed point \\(x_0\\in \\mathcal{C}\\) such that \\(f(x_0)=x_0\\)\nResult hinges on:\n\ncontinuity of \\(f: \\mu \\mapsto \\mu P\\)\nconvexity of \\(\\\\{x \\in R^n | |x|=1 \\\\}\\) (easy to check)\ncompactness of \\(\\\\{x \\in R^n | |x|=1 \\\\}\\)\n\nit is bounded\nand closed (the inverse image of 1 for \\(u\\mapsto |u|\\) which is continuous)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#stationary-distribution-1",
    "href": "lectures/discrete_dynamic_programming/index.html#stationary-distribution-1",
    "title": "Discrete Dynamic Programming",
    "section": "Stationary distribution?",
    "text": "Stationary distribution?\nHow do we compute the stationary distribution?\n\nSimulation\nLinear algebra\nDecomposition"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#simulating-a-markov-chain",
    "href": "lectures/discrete_dynamic_programming/index.html#simulating-a-markov-chain",
    "title": "Discrete Dynamic Programming",
    "section": "Simulating a Markov Chain",
    "text": "Simulating a Markov Chain\n\nVery simple idea:\n\nstart with any \\(\\mu_0\\) and compute the iterates recursively\n\\(\\mu_{n+1}' = \\mu_n' P\\)\nconvergence is linear:\n\n\\(|\\mu_{n+1} - \\mu_n| \\leq |P| |\\mu_n - \\mu_{n-1}|\\)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#using-linear-algebra",
    "href": "lectures/discrete_dynamic_programming/index.html#using-linear-algebra",
    "title": "Discrete Dynamic Programming",
    "section": "Using Linear Algebra",
    "text": "Using Linear Algebra\n\nFind the solution of \\(\\mu'(P-I) = 0\\) ?\n\nnot well defined, 0 is a solution\nwe need to incorporate the constraint \\(\\sum_i(\\mu_i)=1\\)\n\nMethod:\n\nDefine \\(M_{ij} =  \\begin{cases} 1  &\\text{if} & j =0 \\\\\\\\ (P-I)_{ij}  & \\text{if} & j&gt; 1  \\end{cases}\\)\nDefine \\(D_i = \\begin{cases} 1 & \\text{if} & j = 0 \\\\\\\\0 & \\text{if} & j&gt;0 \\end{cases}\\)\nWith a linear algebra solver\n\nlook for a solution \\(\\mu\\) of \\(\\mu' M = D\\)\nor \\(M^{\\prime} \\mu = D\\prime\\)\nif you find a solution, it is unique (theorem)\n\n\nAlternative:\n\nminimize residual squares of overidentified system"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#code-example",
    "href": "lectures/discrete_dynamic_programming/index.html#code-example",
    "title": "Discrete Dynamic Programming",
    "section": "Code example",
    "text": "Code example\n# we use the identity matrix and the \\ operator\nusing LinearAlgebra: I, \\\n# define a stochastic matrix (lines sum to 1)\nP = [  0.9  0.1 0.0  ;\n       0.05 0.9 0.05 ;\n       0.0  0.9 0.1  ]\n# define an auxiliary matrix\nM = P' - I\nM[end,:] .= 1.0\n# define rhs\nR = zeros(3)\nR[end] = 1\n# solve the system\nμ = M\\R\n# check that you have a solution:\n@assert sum(μ) == 1\n@assert all(abs.(μ'P - μ').&lt;1e-10)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#further-comments",
    "href": "lectures/discrete_dynamic_programming/index.html#further-comments",
    "title": "Discrete Dynamic Programming",
    "section": "Further comments",
    "text": "Further comments\n\nKnowledge about the structure of the Markov Chain can help speedup the calculations\nThere are methods for potentially very-large linear system\n\nNewton-Krylov based methods, GMRES\n\nBasic algorithms are easy to implement by hand\nQuantEcon toolbox has very good methods to study markov chains"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#section-2",
    "href": "lectures/discrete_dynamic_programming/index.html#section-2",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "Consider the following problems:\n  \n\n\n\nMonopoly pricing:\n\\[\\max_{q} \\pi(q) - c(q)\\]\n\nShopping problem\n\\[\\max_{\\substack{c_1, c_2 \\\\ p_1 c_1 + p_2 c_2 \\leq B}} U(c_1,c_2)\\]\n\nConsumption Savings\n\\[\\max_{\\substack{c() \\\\ w_{t+1}=(w_t-c(w_t))(1+r)) + y_{t+1}}} E_0 \\sum_t \\beta^t U(c(w_t))\\]\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\nobjective\naction\nstate\ntransition\ntype\n\n\n\n\nmonopoly pricing\nprofit\nchoose quantity to produce\n\n\noptimization\n\n\nshopping problem\nutility\nchoose consumption composition\nbudget \\(B\\)\n\ncomparative statics\n\n\nconsumption/savings\nexpected welfare\nsave or consume\navailable income\nevolution of wealth\ndynamic optimization"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#general-formulation",
    "href": "lectures/discrete_dynamic_programming/index.html#general-formulation",
    "title": "Discrete Dynamic Programming",
    "section": "General Formulation",
    "text": "General Formulation\nMarkov Decision Problem\n\n\n\n\nEnvironment\n\nstates: \\(s \\in S\\)\nactions: \\(x \\in X(s)\\)\ntransitions: \\(\\pi(s'| s, x) \\in S\\)\n\n\\(probability\\) of going to \\(s'\\) in state \\(s\\)…\n… given action \\(x\\)\n\n\n\n\n\n\nReward: \\(r(s,x) \\in R\\)\n\naka felicity, intratemporal utility\n\n\nPolicy: \\(x(): s \\rightarrow x\\in X(s)\\)\n\na.k.a. decision rule\nwe consider deterministic policy\ngiven \\(x()\\), the evolution of \\(s\\) is a Markov process\n\n\\(\\pi(. |s, x())\\) is a distribution for \\(s'\\) over \\(S\\)\nit depends only on \\(s\\)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#objective",
    "href": "lectures/discrete_dynamic_programming/index.html#objective",
    "title": "Discrete Dynamic Programming",
    "section": "Objective",
    "text": "Objective\n\nexpected lifetime reward:\n\nvalue of following policy \\(x()\\) starting from \\(s\\): \\[R(s; x()) =  E_0 \\sum_t^T \\delta^t \\left[ r_t\\right]\\]\n\\(\\delta \\in [0,1[\\): discount factor\nhorizon: \\(T \\in \\\\{N, \\infty\\\\}\\)\n\nvalue of a state \\(s\\)\n\nvalue of following the optimal policy starting from \\(s\\) \\[V(s) = \\max_{ x()} R(s, x())\\]\n\\(V()\\) is the value function (t.b.d.)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#classes-of-dynamic-optimization",
    "href": "lectures/discrete_dynamic_programming/index.html#classes-of-dynamic-optimization",
    "title": "Discrete Dynamic Programming",
    "section": "Classes of Dynamic Optimization",
    "text": "Classes of Dynamic Optimization\n\nThe formulation so far is very general. It encompasses several variants of the problem:\n\nfinite horizon vs infinite horizon\ndiscrete-space problem vs continuous-state space problem\nsome learning problems (reinforcement learning…)\n\nThere are also variants not included:\n\nnon time-separable problems\nnon time-homogenous problems\nsome learning problems (bayesian updating, …)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#finite-horizon-vs-infinite-horizon",
    "href": "lectures/discrete_dynamic_programming/index.html#finite-horizon-vs-infinite-horizon",
    "title": "Discrete Dynamic Programming",
    "section": "Finite horizon vs infinite horizon",
    "text": "Finite horizon vs infinite horizon\n\nRecall objective: \\(V(s; x()) =  \\max E_0\\sum_{t=0}^T \\delta^t \\left[ r(s_t, x_t) \\right]\\)\nIf \\(T&lt;\\infty\\), the decision in the last periods, will be different from the periods before\n\none must find a decision rule \\(\\pi_t()\\) per period\nor, equivalently, add \\(t\\) to the state space: \\(\\tilde{S}=S\\times[0,T]\\)\n\nIf \\(T=\\infty\\), the continuation value of being in state \\(s_t\\) is independent from \\(t\\)\n\n\\[V(s; x()) = E_0 \\max \\sum_ {t=0}^{T_0} \\delta^t \\left[ r(s_t, x_t) \\right] + \\delta^{T_0} E_0  \\sum_ {t=T_0}^{\\infty} \\delta^t \\left[ r(s_t, x_t) \\right]\\]\n\\[ = E_0 \\left[ \\max \\sum_ {t=0}^{T_0} \\delta^t \\left[ r(s_t, x_t) \\right] +  \\delta^{T_0} V(s_ {T_0}; x()) \\right]\\]"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#continuous-vs-discrete",
    "href": "lectures/discrete_dynamic_programming/index.html#continuous-vs-discrete",
    "title": "Discrete Dynamic Programming",
    "section": "Continuous vs discrete",
    "text": "Continuous vs discrete\n\nDiscrete Dynamic Programming (today)\n\ndiscrete states: \\(s \\in {s_1, \\cdots, s_N}\\)\ndiscrete controls: \\(|X(s)|&lt;\\infty\\)\nthere is a finite number of policies, the can be represented exactly\nunless \\(|S|\\) is very large (cf go game)\n\nContinuous problem:\n\n\\(x(s)\\), \\(V(s; \\pi)\\) require an infinite number of coefficients\nsame general approach but different implementation\ntwo main variants:\n\ndiscretize the initial problem: back to DDP\nuse approximation techniques (i.e. interpolation)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#non-time-separable-example",
    "href": "lectures/discrete_dynamic_programming/index.html#non-time-separable-example",
    "title": "Discrete Dynamic Programming",
    "section": "Non time separable example",
    "text": "Non time separable example\n\nFor instance Epstein-Zin preferences: \\[\\max V(;c())\\] where \\[V_t = (1-\\delta) \\frac{c_t^{1-\\sigma}}{1-\\sigma} + \\delta \\left[ E_t V_{t+1}^{\\alpha} \\right]^{\\frac{1}{\\alpha}}\\]\nWhy would you do that?\n\nto disentangle risk aversion and elasticity of intertemporal substitution\nrobust control\n\nYou can still use ideas from Dynamic Programming."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#non-homogenous-preference",
    "href": "lectures/discrete_dynamic_programming/index.html#non-homogenous-preference",
    "title": "Discrete Dynamic Programming",
    "section": "Non homogenous preference",
    "text": "Non homogenous preference\n\nLook at the \\(\\alpha-\\beta\\) model. \\[V_t = \\max \\sum_t^{\\infty} \\beta_t U(c_t)\\] where \\(\\delta_0 = 1\\), \\(\\delta_1=\\alpha\\), \\(\\delta_k=\\alpha\\beta^{k-1}\\)\nMakes the problem time-inconsistent:\n\nthe optimal policy you would choose for the continuation value after \\(T\\) is not the same if you maximize it in expectation from \\(0\\) or at \\(T\\)."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#learning-problems",
    "href": "lectures/discrete_dynamic_programming/index.html#learning-problems",
    "title": "Discrete Dynamic Programming",
    "section": "Learning problems",
    "text": "Learning problems\n\nBayesian learning: Uncertainty about some model parameters\n\nex: variance and return of a stock market\nagent models this uncertainty as a distribution\nagent updates his priors after observing the result of his actions\nactions are taken optimally taken into account the revelation power of some actions\n\nIs it good?\n\nclean: the rational thing to do with uncertainty\nsuper hard: the state-space should contain all possible priors\nmathematical cleanness comes with many assumptions\n\nUsed to estimate rather big (mostly linear) models"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#learning-problems-2",
    "href": "lectures/discrete_dynamic_programming/index.html#learning-problems-2",
    "title": "Discrete Dynamic Programming",
    "section": "Learning problems (2)",
    "text": "Learning problems (2)\n\nReinforcement learning\n\nmodel can be partially or totally unknown\ndecision rule is updated by observing the reward from actions\n\nno priors\n\nsolution does not derive directly from model\n\ncan be used to solve dynamic programming problems\n\n\nGood solutions maximize a criterion similar to lifetime reward but are usually not optimal:\n\nusually evaluated by replaying the game many times\ntradeoff exploration / exploitations"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#finite-horizon-dmdp-1",
    "href": "lectures/discrete_dynamic_programming/index.html#finite-horizon-dmdp-1",
    "title": "Discrete Dynamic Programming",
    "section": "Finite horizon DMDP",
    "text": "Finite horizon DMDP\nWhen \\(T&lt;\\infty\\). With discrete action the problem can be represented by a tree."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#finite-horizon-dmdp-2",
    "href": "lectures/discrete_dynamic_programming/index.html#finite-horizon-dmdp-2",
    "title": "Discrete Dynamic Programming",
    "section": "Finite horizon DMDP",
    "text": "Finite horizon DMDP\n\nIntuition: backward induction.\n\nFind optimal policy \\(x_T(s_T)\\) in all terminal states \\(s_T\\). Set \\(V_T(s_T)\\) equal to \\(r(s_T, \\pi_T)\\)\nFor each state \\(s_{k-1}\\in S\\) find \\(x_{k-1}\\in X(s_{k-1})\\) which maximizes \\[V_{k-1}(s_{k-1}) = \\max_{x_{k-1}(s_{k-1})\\in X(s_{k-1})}r(s_{k-1},x_{k-1}) + \\delta \\underbrace{ \\sum_{s_k\\in S} \\pi(s_k | s_{k-1}, x_{k-1} ) V_k(s_k)} _{ \\textit{expected continuation value} }\\]\n\nPolicies \\(x_0(), ... x_T()\\) are Markov-perfect:\n\nthey maximize utility on all subsets of the “game”\nalso from t=0"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#remarks",
    "href": "lectures/discrete_dynamic_programming/index.html#remarks",
    "title": "Discrete Dynamic Programming",
    "section": "Remarks",
    "text": "Remarks\n\nCan we do better than this naive algorithm?\n\nnot really\nbut we can try to limit \\(S\\) to make the maximization step faster\nexclude a priori some branches in the tree using knowledge of the problem"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#infinite-horizon-dmdp-1",
    "href": "lectures/discrete_dynamic_programming/index.html#infinite-horizon-dmdp-1",
    "title": "Discrete Dynamic Programming",
    "section": "Infinite horizon DMDP",
    "text": "Infinite horizon DMDP\n\nHorizon is infinite: \\[V(s) =  \\max E_0 \\sum_{t=0}^{\\infty} \\delta^t r(s_t, x_t) \\]\nIntuition:\n\nlet’s consider the finite horizon version \\(T&lt;\\infty\\) and \\(T &gt;&gt; 1\\)\ncompute the solution, increase \\(T\\) until the solution doesn’t change\nin practice: take an initial guess for \\(V_{T}\\) then compute optimal \\(V_{T-1}\\), \\(V_{T_2}\\) and so on, until convergence of the \\(V\\)s"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#infinite-horizon-dmdp-2",
    "href": "lectures/discrete_dynamic_programming/index.html#infinite-horizon-dmdp-2",
    "title": "Discrete Dynamic Programming",
    "section": "Infinite horizon DMDP (2)",
    "text": "Infinite horizon DMDP (2)\n\nThis is possible, it’s called Successive Approximation or Value Function Iteration\n\nhow fast does it converge? linearly\ncan we do better? yes, quadratically\n\nwith howard improvement steps"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#successive-approximation",
    "href": "lectures/discrete_dynamic_programming/index.html#successive-approximation",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation",
    "text": "Successive Approximation\n\nConsider the decomposition: \\[V(s; x()) = E_0 \\sum_{t=0}^{\\infty} \\delta^t r(s_t, x_t) = E_0 \\left[ r(s, x(s)) + \\sum_{t=1}^{\\infty} \\delta^t r(s_t, x_t) \\right]\\]\n\nor\n\\[V(s; x()) =  r(s, x(s)) + \\delta \\sum_{s'} p(s'|s,x(s)) V(s'; x()) \\]"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#successive-approximation-2",
    "href": "lectures/discrete_dynamic_programming/index.html#successive-approximation-2",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation (2)",
    "text": "Successive Approximation (2)\n\nTaking continuation value as given we can certainly improve the value in every state \\(\\tilde{V}\\) by choosing \\(\\tilde{x}()\\) so as to maximize \\[\\tilde{V}(s; \\tilde{x}(), x()) =  r(s, \\tilde{x}(s)) + \\delta \\sum_{s'} \\pi(s'|s,\\tilde{x}(s) )V(s'; x()) \\]\nBy construction: \\(\\forall s, \\tilde{V}(s, \\tilde{x}(), x()) &gt; {V}(s, x())\\)\n\nit is an improvement step\n\nCan \\({V}(s, \\tilde{x}())\\) be worse for some states than \\({V}(s, x())\\) ?\n\nactually no"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#bellman-equation",
    "href": "lectures/discrete_dynamic_programming/index.html#bellman-equation",
    "title": "Discrete Dynamic Programming",
    "section": "Bellman equation",
    "text": "Bellman equation\n\nIdea:\n\nit should not be possible to improve upon the optimal solution.\nHence the optimal value \\(V\\) and policy \\(x^{\\star}\\) should satisfy: \\[\\forall s\\in S, V(s) = \\max_{y(s)} r(s, y(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, y(s)) V(s^{\\prime})\\] with the maximum attained at \\(x(s)\\).\n\nThis is referred to as the Bellman equation.\nConversely, it is possible to show that a solution to the Bellman equation is also an optimal solution to the initial problem."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#bellman-operator",
    "href": "lectures/discrete_dynamic_programming/index.html#bellman-operator",
    "title": "Discrete Dynamic Programming",
    "section": "Bellman operator",
    "text": "Bellman operator\n\nThe function \\(G\\) is known as the Bellman operator: \\[G: V \\rightarrow \\max_{y(s)} r(s, y(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, y(s)) V(s^{\\prime})\\]\nDefine sequence \\(V_n = G(V_{n-1})\\)\n\nit goes back in time\nbut is not the time-iteration operator\n\nOptimal value is a fixed point of G\nDoes \\(G\\) converges to it ? Yes, if \\(G\\) is a contraction mapping."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#blackwells-theorem",
    "href": "lectures/discrete_dynamic_programming/index.html#blackwells-theorem",
    "title": "Discrete Dynamic Programming",
    "section": "Blackwell’s theorem",
    "text": "Blackwell’s theorem\n\nLet \\(X\\subset R^n\\) and let \\(\\mathcal{C}(X)\\) be a space of bounded functions \\(f: X\\rightarrow  R\\), with the sup-metric. \\(B: \\mathcal{C}(X)\\rightarrow \\mathcal{C}(X)\\) be an operator satisfying two conditions:\n\n(monotonicity) if \\(f,g \\in \\mathcal{C}(X)\\) and \\(\\forall x\\in X, f(x)\\leq g(x)\\) then\n\n\\(\\forall x \\in X (Bf)(x)\\leq(Bg)(x)\\)\n\n(discounting) there exists some \\(\\delta\\in]0,1[\\) such that: \\(B.(f+a)(x)\\leq (B.f)(x) + \\delta a, \\forall f \\in \\mathcal{C}(X), a\\geq 0, x\\in X\\)\n\nThen \\(B\\) is a contraction mapping with modulus \\(\\delta\\)."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#successive-approximation-1",
    "href": "lectures/discrete_dynamic_programming/index.html#successive-approximation-1",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation",
    "text": "Successive Approximation\n\nUsing the Blackwell’s theorem, we can prove the Bellman operator is a contraction mapping.\nThis justifies the Value Function Iteration algorithm:\n\nchoose an initial \\(V_0\\)\ngiven \\(V_n\\) compute \\(V_{n+1} = G(V_n)\\)\niterate until \\(|V_{n+1}- V_n|\\leq \\eta\\)\n\nPolicy rule is deduced from \\(V\\) as the maximand in the Bellman step"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#successive-approximation-2-1",
    "href": "lectures/discrete_dynamic_programming/index.html#successive-approximation-2-1",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation (2)",
    "text": "Successive Approximation (2)\nAssume that \\(X\\) is finite.\n\nNote that convergence of \\(V_n\\) is geometric\nBut \\(x_n\\) converges after a finite number of iteration.\n\nsurely the latest iterations are suboptimal\nthey serve only to evaluate the value of \\(x\\)\n\nIn fact:\n\n\\(V_n\\) is never the value of \\(x_n()\\)\nshould we try to keep both in sync?"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#policy-iteration-for-dmdp",
    "href": "lectures/discrete_dynamic_programming/index.html#policy-iteration-for-dmdp",
    "title": "Discrete Dynamic Programming",
    "section": "Policy iteration for DMDP",
    "text": "Policy iteration for DMDP\n\nChoose initial policy \\(x_0()\\)\nGiven initial guess \\(x_n()\\)\n\ncompute the value function \\(V_n=V( ;x_n)\\) which satisfies\n\\(\\forall s,  V_n(s) = r(s, x_n(s)) + \\delta \\sum_{s'} \\pi(s'| s, x_n(s)) V_n(s')\\)\nimprove policy by maximizing in \\(x_n()\\) \\[\\max_{x_n()} r(s, x_n(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, x_n(s)) V_{n-1}(s^{\\prime})\\]\n\nRepeat until convergence, i.e. \\(x_n=x_{n+1}\\)\nOne can show the speed of convergence (for \\(V_n\\)) is quadratic\n\nit corresponds the Newton-Raphson steps applied to \\(V\\rightarrow G(V)-V\\)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#how-do-we-compute-the-value-of-a-policy",
    "href": "lectures/discrete_dynamic_programming/index.html#how-do-we-compute-the-value-of-a-policy",
    "title": "Discrete Dynamic Programming",
    "section": "How do we compute the value of a policy?",
    "text": "How do we compute the value of a policy?\n\nGiven \\(x_n\\), goal is to find \\(V_n(s)\\) in \\[\\forall s,  V_n(s) = r(s, x_n(s)) + \\delta \\sum_{s'} \\pi(s'| s, x_n(s)) V_n(s')\\]\nThree approaches:\n\nsimulate the policy rule and compute \\(E\\left[ \\sum_t \\delta^t r(s_t, x_t) \\right]\\) with Monte-Carlo draws\nsuccessive approximation:\n\nput \\(V_k\\) in the rhs and recompute the lhs \\(V_{k+1}\\), replace \\(V_k\\) by \\(V_{k+1}\\) and iterate until convergence\n\nsolve a linear system in \\(V_n\\)\n\nFor 2 and 3 it is useful to constuct a linear operator \\(M\\) such that \\(V_{n+1} = R_n + \\delta M_n .  V_n\\)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#idea",
    "href": "lectures/discrete_dynamic_programming/index.html#idea",
    "title": "Discrete Dynamic Programming",
    "section": "Idea",
    "text": "Idea\n\nMcCall model:\n\nwhen should an unemployed person accept a job offer?\nchoice between:\n\nwait for a better offer (and receive low unemp. benefits)\naccept a suboptimal job offer\n\n\nWe present a variant of it, with a small probability of loosing a job."
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#formalization",
    "href": "lectures/discrete_dynamic_programming/index.html#formalization",
    "title": "Discrete Dynamic Programming",
    "section": "Formalization",
    "text": "Formalization\n\nWhen unemployed in date, a job-seeker\n\nconsumes unemployment benefit \\(c = \\underline{c}\\)\nreceives in every date \\(t\\) a job offer \\(w\\)\n\n\\(w\\) is i.i.d.,\ntakes values \\(w_1, w_2, w_3\\) with probabilities \\(p_1, p_2, p_3\\)\n\nif job-seeker accepts, becomes employed at rate \\(w\\) in the next period\nelse he stays unemployed\n\nWhen employed at rate \\(w\\)\n\nworker consumes salary \\(c = w\\)\nwith small probability \\(\\lambda&gt;0\\) looses his job:\n\nstarts next period unemployed\n\notherwise stays employed at same rate\n\nObjective: \\(\\max E_0 \\left\\{ \\sum \\beta^t \\log(c^t) \\right\\}\\)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#states-reward",
    "href": "lectures/discrete_dynamic_programming/index.html#states-reward",
    "title": "Discrete Dynamic Programming",
    "section": "States / reward",
    "text": "States / reward\n\nWhat are the states?\n\nemployement status: Unemployed / Employed\nif Unemployed:\n\nthe level \\(w\\in {w_1, w_2, w_3}\\) of the salary that is currently proposed\n\nif Employed:\n\nthe level \\(w\\in {w_1, w_2, w_3}\\) at which worker was hired\n\ncurrent state, can be represented by a 2x3 index\n\nWhat are the actions?\n\nif Unemployed:\n\nreject (false) / accept (true)\n\nif Employed: None\nactions (when unemployed) are represented by a 3 elements binary vector\n\nWhat is the (intratemporal) reward?\n\nif Unemployed: \\(U(c)\\)\nif Employed at rate w: \\(U(w)\\)\nhere it doesn’t depend on the action"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#value-function",
    "href": "lectures/discrete_dynamic_programming/index.html#value-function",
    "title": "Discrete Dynamic Programming",
    "section": "Value function",
    "text": "Value function\n\\(\\newcommand{\\E}{\\mathbb{E}}\\)\n\nWhat is the value of being in a given state?\nIf Unemployed, facing current offer \\(w\\):\n\\[V^U(w) = U(\\underline{c}) + \\max_{a} \\begin{cases} \\beta V^E(w) & \\text{if $a(w)$ is true} \\\\ \\beta  E_{w'}\\left[ V^U(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\]\nIf Employed, at rate \\(w\\) \\[V^E(w) = U(w) +  (1-\\lambda) \\beta V^E(w) +  \\lambda \\beta E_{w'}\\left[ V^U(w^{\\prime}) \\right] \\]\nWe can represent value as two functions \\(V^U\\) and \\(V^E\\) of the states as\n\ntwo vectors of Floats, with three elements (recall: value-function is real valued)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#value-function-iteration",
    "href": "lectures/discrete_dynamic_programming/index.html#value-function-iteration",
    "title": "Discrete Dynamic Programming",
    "section": "Value function iteration",
    "text": "Value function iteration\n\nTake a guess for value function \\(\\tilde{V^E}\\), \\(\\tilde{V^U}\\), tomorrow\nUse it to compute value function today: \\[V^U(w) = U(\\underline{c}) + \\max_{a(w)} \\begin{cases} \\beta \\tilde{V}^E(w) & \\text{if $a(w)$ is true} \\\\ \\beta  E_{w'}\\left[ \\tilde{V}^U(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V^E(w) = U(w) +  (1-\\lambda) \\beta \\tilde{V}^E(w) +  \\lambda \\beta E_{w'}\\left[\\tilde{V}^U(w^{\\prime}) \\right] \\]\n\\((\\tilde{V}^E, \\tilde{V}^U)\\mapsto (V^E, V^U)\\) is one value iteration step\nNote that we don’t have to keep track of policies tomorrow\n\nall information about future decisions is contained in \\(\\tilde{V}^E, \\tilde{V}^U\\)\nbut we can keep track of current policy: \\(a(w): \\arg\\max \\cdots\\)"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#value-evaluation",
    "href": "lectures/discrete_dynamic_programming/index.html#value-evaluation",
    "title": "Discrete Dynamic Programming",
    "section": "Value evaluation",
    "text": "Value evaluation\n\nSuppose we take a policy \\(a(w)\\) as given. What is the value of following this policy forever?\nThe value function \\(V_a^E\\), \\(V_a^U\\) satisfies \\[V_a^U(w) = U(\\underline{c}) + \\begin{cases} \\beta {V}^E_a(w) & \\text{if $a(w)$ is true} \\\\ \\beta  E_{w'}\\left[ {V}^U_a(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V_a^E(w) = U(w) +  (1-\\lambda) \\beta {V}^E_a(w) +  \\lambda \\beta E_{w'}\\left[{V}^U_a(w^{\\prime}) \\right] \\]\nNote the absence of the max function: we don’t reoptimize"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#value-evaluation-2",
    "href": "lectures/discrete_dynamic_programming/index.html#value-evaluation-2",
    "title": "Discrete Dynamic Programming",
    "section": "Value evaluation (2)",
    "text": "Value evaluation (2)\n\nHow do you compute value of policy \\(a(w)\\) recursively?\nIterate: \\((\\tilde{V}^E_a, \\tilde{V}^U)\\mapsto (V^E_a, V^U_a)\\) \\[V_a^U(w) \\leftarrow U(\\underline{c}) + \\begin{cases} \\beta \\tilde{V}^E_a(w) & \\text{if $a(w)$ is true} \\\\ \\beta  E_{w'}\\left[ \\tilde{V}^U_a(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V_a^E(w) \\leftarrow U(w) +  (1-\\lambda) \\beta \\tilde{V}^E_a(w) +  \\lambda \\beta E_{w'}\\left[\\tilde{V}^U_a(w^{\\prime}) \\right] \\]\nNote the absence of the max function:\n\nwe don’t reoptimize\nwe we keep the same policy all along"
  },
  {
    "objectID": "lectures/discrete_dynamic_programming/index.html#policy-iteration",
    "href": "lectures/discrete_dynamic_programming/index.html#policy-iteration",
    "title": "Discrete Dynamic Programming",
    "section": "Policy iteration",
    "text": "Policy iteration\n\nstart with policy \\(a(w)\\)\nevaluate the value of this policy \\(V^E_a, V^U_a\\)\ncompute the optimal policy \\(a(w)\\) in the Bellman iteration\n\nhere: \\(a(w) = \\arg\\max_{a(w)} \\begin{cases} \\beta \\tilde{V}^E(w)\\\\ \\beta  E_{a'}\\left[ \\tilde{V}^U(a^{\\prime}) \\right] \\end{cases}\\)\n\niterate until \\(a(w)\\) converges"
  },
  {
    "objectID": "lectures/perturbation/index.html#today",
    "href": "lectures/perturbation/index.html#today",
    "title": "Perturbation Analysis",
    "section": "Today",
    "text": "Today\nStudy Neoclassical Model of Growth with Deterministic Productivity Shocks\n\nDerive First Order Conditions\nComputing Derivatives Numerically\nSolution Method\n\nLinear Time Iteration\n\nImplementation"
  },
  {
    "objectID": "lectures/perturbation/index.html#neoclassical-growth-model",
    "href": "lectures/perturbation/index.html#neoclassical-growth-model",
    "title": "Perturbation Analysis",
    "section": "Neoclassical Growth Model",
    "text": "Neoclassical Growth Model\n\n\n\nTransition Equation \\[\\begin{eqnarray}\nk_t & = & (1-\\delta) k_{t-1} + i_{t-1} \\\\\nz_t & = & \\rho z_{t-1}\n\\end{eqnarray}\n\\]\nDefinition: \\[c_t = \\exp(z_t) k_t^\\alpha - i_t\\]\nControl \\(i_t\\in[0, \\exp(z_t)k_t^\\alpha[\\)\n\nor equivalently \\(c_t \\in ]0, \\exp(z_t) k_t^{\\alpha}]\\)\n\nObjective: \\[\\max_{i_t} \\sum_{t\\geq0} \\beta^t U(c_t)\\]\n\n\n\nCalibration:\n\n\\(\\beta = 0.96\\)\n\\(\\delta = 0.1\\)\n\\(\\gamma = 4.0\\)\n\\(\\alpha = 0.3\\)\n\\(U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}\\)"
  },
  {
    "objectID": "lectures/perturbation/index.html#deriving-first-order-conditions-1",
    "href": "lectures/perturbation/index.html#deriving-first-order-conditions-1",
    "title": "Perturbation Analysis",
    "section": "Deriving First Order Conditions",
    "text": "Deriving First Order Conditions\n\nTwo methods:\n\nBellman:\n\nOptimality Condition\nEnveloppe Condition\n\nLagrangian:\n\nWe will use the lagrangian version"
  },
  {
    "objectID": "lectures/perturbation/index.html#lagrangian",
    "href": "lectures/perturbation/index.html#lagrangian",
    "title": "Perturbation Analysis",
    "section": "Lagrangian",
    "text": "Lagrangian\n\nInitial Conditions (predetermined states): \\(z_0\\), \\(k_0\\)\nProblem: \\[V(z_0, k_0) = \\max_{\\begin{matrix}i_0, i_1, i_2, \\cdots \\\\c_0, c_1, c_2 \\cdots \\\\ k_1, k_2, \\cdots\\end{matrix}} \\sum_{t \\geq 0}\\beta^t U(c_t)\\]\n\n\\[\\text{s.t.}\\forall t\\geq 0, \\; \\; \\begin{eqnarray}\n\\mu_t:\\quad &  0 & \\leq & i_t  \\\\\n\\nu_t:\\quad &  i_t & \\leq & \\exp(z_t) k_t^{\\alpha} \\\\\n\\lambda_t:\\quad &  i_t & = & \\exp(z_t) k_t^{\\alpha} - c_t\\\\\nq_t:\\quad &  k_{t+1} & = & (1-\\delta) k_{t} + i_{t}\n\\end{eqnarray}\\]\n\nLagrangian: \\[\n\\mathcal{L} (z_0, k_0) = \\sum_{t \\geq 0} \\beta^t\\left\n\\{ U(c_t) + \\mu_t \\left( i_t \\right) + \\nu_t \\left(\\exp(z_t)k_t^{\\alpha} - i_t \\right) + \\lambda_t \\left(\\exp(z_t) k_t^{\\alpha}  - i_t -c_t \\right)  + q_t \\left( (1-\\delta) k_{t} + i_{t} - k_{t+1} \\right) \\right\\}\n\\]"
  },
  {
    "objectID": "lectures/perturbation/index.html#lagrangian-1",
    "href": "lectures/perturbation/index.html#lagrangian-1",
    "title": "Perturbation Analysis",
    "section": "Lagrangian",
    "text": "Lagrangian\n\n\n\nWe maximize the lagrangian to get:\n\n\\[\\begin{eqnarray}\n\\forall t\\geq0 & \\frac{\\partial \\mathcal{L}}{\\partial i_t} & = & 0 \\\\\n& \\frac{\\partial \\mathcal{L}}{\\partial c_t} & = & 0 \\\\\n& \\frac{\\partial \\mathcal{L}}{\\partial k_{t+1}} & = & 0\n\\end{eqnarray}\\]\n\nIt is important to note that we don’t differentiate with respect to a predetermined state\n\ncheck that you don’t differentiate w.r.t. \\(k_0\\)\n\nIt looks like the first order condition added four new variables \\(\\mu_t\\),\\(\\nu_t\\), \\(\\lambda_t\\), \\(q_t\\)\n\n\n\nLuckily these variables are associated to slackness conditions.\n\n\n\n\n\\(\\mu_t \\geq 0\\)\n\\(i_t \\geq 0\\)\n\n\n\\(\\nu_t \\geq 0\\)\n\\(\\exp(z_t) k_t^{\\alpha}-i_t \\geq 0\\)\n\n\n\\(q_t \\geq 0\\)\n\\((1-\\delta) k_{t} + i_{t} - k_{t+1} \\geq 0\\)\n\n\n\\(\\lambda_t \\geq 0\\)\n\\(\\exp(z_t) k_t^{\\alpha}  - i_t -c_t = 0\\)\n\n\n\n\nThe Karush-Kuhn-Tucker states, that for each slackness condition, at any time, either\n\nthe lagrangian is 0 and it disappears from the F.O.C.s\nor it is not 0 and the associated constraint adds another condition"
  },
  {
    "objectID": "lectures/perturbation/index.html#eliminating-constraints",
    "href": "lectures/perturbation/index.html#eliminating-constraints",
    "title": "Perturbation Analysis",
    "section": "Eliminating constraints",
    "text": "Eliminating constraints\n\n\n\n\n\n\\(\\mu_t \\geq 0\\)\n\\(i_t \\geq 0\\)\n\n\n\\(\\nu_t \\geq 0\\)\n\\(\\exp(z_t) k_t^{\\alpha}-i_t \\geq 0\\)\n\n\n\\(q_t\\)\n\\((1-\\delta) k_{t} + i_{t} - k_{t+1} = 0\\)\n\n\n\\(\\lambda_t\\)\n\\(\\exp(z_t) k_t^{\\alpha}  - i_t -c_t = 0\\)\n\n\n\n\nIn general slackness conditions can be occasionally binding\nFor perturbation analysis, we need constraints to be always (or never binding)\n\n\nLet’s review them:\n\n\\(\\nu_t\\): it is equivalent to \\(c_t\\geq 0\\)\n\nwe necessarily have \\(c_t&gt;0\\) since \\(U^{\\prime}(0)=\\infty\\)\nhence \\(\\nu_t=0\\)\n\n\\(\\mu_t\\): it states \\(k_{t+1}\\geq 0\\)\n\nif \\(k_{t+1}=0\\), then \\(c_{t+1}\\). We can conclude \\(k_{t+1}&gt;0\\)\nhence \\(\\mu_t=0\\)\n\nfor multipliers associated to an equality constraint, we always keep the system\n\nmultiplier can have any sign\n\ninequality formulation is sometimes found too:\n\n\\(c_t \\leq \\exp(z_t) k_t^{\\alpha}  - i_t\\) ( you can destroy production insead of eating or investing it)\n\\(k_{t+1} \\leq (1-\\delta) k_{t} + i_{t}\\) (you can destroy capital instead of investing it)"
  },
  {
    "objectID": "lectures/perturbation/index.html#first-order-model",
    "href": "lectures/perturbation/index.html#first-order-model",
    "title": "Perturbation Analysis",
    "section": "First order model",
    "text": "First order model\n\nOptimality Condition: \\[\\beta  \\left[ \\frac{\\left(c_{t+1}\\right)^{-\\gamma}}{\\left(c_t\\right)^{-\\gamma}} \\left( (1-\\delta + \\alpha exp(z_{t+1})k_{t+1}^{\\alpha -1}) \\right)\\right] = 1\\]\n\nTakes into account the fact that \\(c_t&gt;0\\).\n\nDefinition: \\[c_t = exp(z_t) k_t^\\alpha - i_t\\]\nTransition: \\[k_t = (1-\\delta) k_{t-1} + i_{t-1}\\] \\[z_t = \\rho z_{t-1}\\]"
  },
  {
    "objectID": "lectures/perturbation/index.html#steady-state",
    "href": "lectures/perturbation/index.html#steady-state",
    "title": "Perturbation Analysis",
    "section": "Steady-State",
    "text": "Steady-State\n\n\n\nSteady-State: \\(\\overline{i}, \\overline{k}, \\overline{z}\\) such that:\n\n\\(z_{t+1}=z_t=\\overline{z}\\)\n\\(k_{t+1}=k_t=\\overline{k}\\)\n\\(i_{t+1}=i_t=\\overline{i}\\)\n\\(c_{t+1}=c_t=\\overline{c}\\)\n\n…satisfy the first order conditions\n…i.e.\n\n\\[\n\\begin{eqnarray}\n1 & = & \\beta   \\left( (1-\\delta + \\alpha {\\overline{k}}^{\\alpha -1}) \\right)  \\\\\n\\overline{k} & = & (1-\\delta) \\overline{k} + \\overline{i} \\\\\n\\overline{z} & = & \\rho \\overline{z} \\\\\n\\overline{c} & = & \\overline{k}^{\\alpha} - \\overline{i}\n\\end{eqnarray}\n\\]\n\n\nSolution?\n\nclosed-form\nnumerical\n\nHere we can get a closed form:\n\n\\[\\begin{eqnarray}\n\\overline{k} & = & \\left( \\frac{\\frac{1}{\\beta}-(1-\\delta)}{\\alpha} \\right)^{\\frac{1}{\\alpha - 1}} \\\\\n\\overline{i} & = & \\delta \\overline{k} \\\\\\\n\\overline{z} & = & 0 \\\\\n\\overline{c} & = & \\overline{k}^{\\alpha} - \\overline{i}\n\\end{eqnarray}\\]"
  },
  {
    "objectID": "lectures/perturbation/index.html#perturbation-analysis",
    "href": "lectures/perturbation/index.html#perturbation-analysis",
    "title": "Perturbation Analysis",
    "section": "Perturbation Analysis",
    "text": "Perturbation Analysis\n\n\n\nWrite all variables in deviation to the steady-state: \\[z_{t}=\\overline{z} + \\Delta z_t\\] \\[k_{t}=\\overline{k} + \\Delta k_t\\] \\[i_{t}=\\overline{i} + \\Delta i_t\\] \\[c_{t}=\\overline{c} + \\Delta c_t\\]\n\nRemark: some smart economists use log-deviations (i.e. \\(x_t = \\overline{x} \\hat{x}_t\\) to make computations easier)\n\n\n\n\nReplace in the system \\[\\beta  \\left[ \\frac{\\left(\\overline{c}+ \\Delta c_{t+1}\\right)^{-\\gamma}}{\\left(\\overline{c} + \\Delta c_t\\right)^{-\\gamma}} \\left( (1-\\delta + \\alpha (\\overline{k} + \\Delta k_{t+1})^{\\alpha -1}) \\right)\\right] = 1\\] \\[\\overline{c} + \\Delta c_t = (\\overline{k}+ \\Delta k_t)^\\alpha - \\overline{i} - \\Delta i_t\\] \\[\\overline{k} + \\Delta k_t = (1-\\delta) (\\overline{k}+ \\Delta k_{t-1}) + \\overline{i }+ \\Delta i_{t-1}\\] \\[\\overline{z }+ \\Delta z_t = \\overline{z}+ \\Delta \\rho z_{t-1}\\]\nDifferentiate…\n(if we want to limit the number of equations, we can replace \\(c_t\\) by its value)"
  },
  {
    "objectID": "lectures/perturbation/index.html#result",
    "href": "lectures/perturbation/index.html#result",
    "title": "Perturbation Analysis",
    "section": "Result:",
    "text": "Result:\n\nOptimality conditions \\[\\begin{bmatrix} . & . & . & . \\\\ \\end{bmatrix} \\begin{bmatrix} \\Delta i_t \\\\ \\Delta c_t  \\\\ \\Delta k_t \\\\ \\Delta z_t \\end{bmatrix} = \\begin{bmatrix} . & . & . \\\\ \\end{bmatrix} \\begin{bmatrix} \\Delta i_{t+1} \\\\  \\Delta c_{t+1} \\\\ \\Delta k_{t+1} \\\\ \\Delta z_{t+1} \\end{bmatrix} \\]\nTransition \\[ \\begin{bmatrix} \\Delta k_t \\\\ \\Delta z_t \\end{bmatrix} = \\begin{bmatrix} . & .  \\\\ . & . \\end{bmatrix} \\begin{bmatrix} \\Delta k_{t-1} \\\\ \\Delta z_{t-1} \\end{bmatrix}  + \\begin{bmatrix} . \\end{bmatrix} \\begin{bmatrix}\\Delta i_{t-1}\\end{bmatrix}\\]"
  },
  {
    "objectID": "lectures/perturbation/index.html#julia-console",
    "href": "lectures/perturbation/index.html#julia-console",
    "title": "Perturbation Analysis",
    "section": "Julia Console",
    "text": "Julia Console\n\nAccessible:\n\nfrom a good terminal\nfrom VSCode panel\n\nFour modes:\n\n``: REPL (read-eval-print)\n?: Help\n]: Package Management\n;: System Console"
  },
  {
    "objectID": "lectures/perturbation/index.html#julia-package-ecosystem",
    "href": "lectures/perturbation/index.html#julia-package-ecosystem",
    "title": "Perturbation Analysis",
    "section": "Julia package ecosystem",
    "text": "Julia package ecosystem\n\nLarge package ecosystem\nFairly good quality native code\nWrappers to low-level / foreign language libraries\n\nC: ccall\nFortran: fcall\nPython: PyCall"
  },
  {
    "objectID": "lectures/perturbation/index.html#how-do-you-install-pckages",
    "href": "lectures/perturbation/index.html#how-do-you-install-pckages",
    "title": "Perturbation Analysis",
    "section": "How do you install pckages?",
    "text": "How do you install pckages?\n\nShort and wrong answer: ] add PackageName\nBetter answer:\n\na project environment specifies all dependencies for a project\n\ninformations are contained in Project.toml\n\nchange directory to the right project\n\n; cd path_to_the_right_project\nyou can check where you are ; pwd (print working directory)\n\nactivate environment:\n\n] activate . (. universally means current director)\n\nadd desired package:\n\n] add PackageName\n\nwhen you restart work on a project activate, it again, to ensure you have the right dependencies"
  },
  {
    "objectID": "lectures/perturbation/index.html#main-approaches",
    "href": "lectures/perturbation/index.html#main-approaches",
    "title": "Perturbation Analysis",
    "section": "Main approaches",
    "text": "Main approaches\n\nBack to our problem, how to we differentiate the model?\nMain approaches:\n\nManual\nFinite Differences\nSymbolic Differentiation\nAutomatic Differentiation"
  },
  {
    "objectID": "lectures/perturbation/index.html#manual-differentiation",
    "href": "lectures/perturbation/index.html#manual-differentiation",
    "title": "Perturbation Analysis",
    "section": "Manual Differentiation",
    "text": "Manual Differentiation\n\nTrick:\n\nnever use \\(\\frac{d}{dx} \\frac{u(x)}{v(x)} = \\frac{u'(x)v(x)-u(x)v'(x)}{v(x)^2}\\)\n\ntoo error prone\n\nuse instead \\[\\frac{d}{dx} {u(x)v(x)} = {u'(x)v(x)+u(x)v'(x)}\\] and \\[\\frac{d}{dx} \\frac{1}{u(x)} = -\\frac{u^{\\prime}}{u(x)^2}\\]\n\nYou can get easier calculations (in some cases) by using log-deviation rules"
  },
  {
    "objectID": "lectures/perturbation/index.html#finite-differences",
    "href": "lectures/perturbation/index.html#finite-differences",
    "title": "Perturbation Analysis",
    "section": "Finite Differences",
    "text": "Finite Differences\n\n\n\n\nChoose small \\(\\epsilon&gt;0\\), typically \\(\\sqrt{ \\textit{machine eps}}\\)\n\ncheck eps()\n\nForward Difference scheme:\n\n\\(f'(x) \\approx \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}\\)\nprecision: \\(O(\\epsilon)\\)\nbonus: if \\(f(x+\\epsilon)\\) can compute \\(f(x)-f(x-\\epsilon)\\) instead (Backward)\n\nCentral Difference scheme:\n\nFinite \\(f'(x) \\approx \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon}\\)\naverage of forward and backward\nprecision: \\(O(\\epsilon^2)\\)\n\n\n\n\nTwo packages FiniteDiff and FiniteDifferences.\nExample:\nusing FiniteDiff\n\ng(x) = [x[1]^2 x[2]^3]\nFiniteDiff.finite_difference_jacobian(g, [0.1, 0.2])"
  },
  {
    "objectID": "lectures/perturbation/index.html#finite-differences-higher-order",
    "href": "lectures/perturbation/index.html#finite-differences-higher-order",
    "title": "Perturbation Analysis",
    "section": "Finite Differences: Higher order",
    "text": "Finite Differences: Higher order\n\nCentral formula: \\[\n\\begin{aligned}\nf''(x) & \\approx & \\frac{f'(x)-f'(x-\\epsilon)}{\\epsilon} \\approx \\frac{(f(x+\\epsilon))-f(x))-(f(x)-f(x-\\epsilon))}{\\epsilon^2} \\\\ & = & \\frac{f(x+\\epsilon)-2f(x)+f(x-\\epsilon)}{\\epsilon^2}\n\\end{aligned}\n\\]\n\nprecision: \\(o(\\epsilon)\\)\n\nGeneralizes to higher order but becomes more and more innacurate"
  },
  {
    "objectID": "lectures/perturbation/index.html#symbolic-differentiation",
    "href": "lectures/perturbation/index.html#symbolic-differentiation",
    "title": "Perturbation Analysis",
    "section": "Symbolic Differentiation",
    "text": "Symbolic Differentiation\n\n\n\nmanipulate the tree of algebraic expressions\n\nimplements various simplification rules\n\nrequires mathematical expression\ncan produce mathematical insights\nsometimes inaccurate:\n\ncf: \\(\\left(\\frac{1+u(x)}{1+v(x)}\\right)^{100}\\)\n\n\n\nTwo main libraries:\n\nSymEngine.jl\n\nfast symbolic calculation\nmature C++ engine\n\nSymbolics.jl:\n\npure julia\nfinite difference\nsymbolic calculation\n\n\nExample using Symbolics:\nusing Symbolics\n@variables a x b y\neq = a*sin(x) + b*cos(y)\nSymbolics.derivative(eq, x)"
  },
  {
    "objectID": "lectures/perturbation/index.html#automatic-differentiation",
    "href": "lectures/perturbation/index.html#automatic-differentiation",
    "title": "Perturbation Analysis",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\n\nAutomatic Differentiation\n\ndoes not provide mathematical insights but solves the other problems\ncan differentiate any piece of code\ntwo flavours\n\nforward accumulation\nreverse accumulation"
  },
  {
    "objectID": "lectures/perturbation/index.html#automatic-source-code-rewrite",
    "href": "lectures/perturbation/index.html#automatic-source-code-rewrite",
    "title": "Perturbation Analysis",
    "section": "Automatic source code rewrite",
    "text": "Automatic source code rewrite\n\n\nConsider this simple function\nfunction f(x::Number)\n\n    a = x + 1\n    b = x^2\n    c = sin(a) \n    d = c + b\n\n    return d\n\nend\n\nWe can rewrite the code as follows:\nfunction f(x::Number)\n\n    # x is an argument\n    x_dx = 1.0\n\n    a = x + 1\n    a_dx = x_dx\n\n    b = x^2\n    b_dx = 2*x*x_dx\n\n    c = sin(a)\n    c_x = cos(a)*a_dx\n\n    d = c + b\n    d_x = c_dx + b_dx\n\n    return (d, d_x)\nend"
  },
  {
    "objectID": "lectures/perturbation/index.html#dual-numbers-operator-overloading",
    "href": "lectures/perturbation/index.html#dual-numbers-operator-overloading",
    "title": "Perturbation Analysis",
    "section": "Dual numbers: operator overloading",
    "text": "Dual numbers: operator overloading\nInstead of rewriting source code, we can use dual numbers to perform exactly the same calculations.\nstruct DN&lt;:Number\n    x::Float64\n    dx::Float64\nend\n\n+(a::DN,b::DN) = DN(a.x+b.x, a.dx+b.dx)\n-(a::DN,b::DN) = DN(a.x-b.x, a.dx-b.dx)\n*(a::DN,b::DN) = DN(a.x*b.x, a.x*b.dx+a.dx*b.x)\n/(a::DN,b::DN) = DN(a.x/b.x, (a.dx*b.x-a.x*b.dx)/b.dx^2)\n\n...\nTry it on function f What do we miss ?"
  },
  {
    "objectID": "lectures/perturbation/index.html#dual-numbers",
    "href": "lectures/perturbation/index.html#dual-numbers",
    "title": "Perturbation Analysis",
    "section": "Dual numbers",
    "text": "Dual numbers\nThis approach (and automatic differentiation in general) is compatible with control flow operations (if, while, …)\nLet’s see it with the dual numbers defined by ForwardDiff library:\nimport ForwardDiff: Dual\n\nx = Dual(1.0, 1.0)\na = 0.5*x\nb = sum([(x)^i/i*(-1)^(i+1) for i=1:5000])\n# compare with log(1+x)\n\ngeneralizes nicely to gradient computations\n\nx = Dual(1.0, 1.0, 0.0)\ny = Dual(1.0, 0.0, 1.0)\nexp(x) + log(y)"
  },
  {
    "objectID": "lectures/perturbation/index.html#automatic-differentiation-2",
    "href": "lectures/perturbation/index.html#automatic-differentiation-2",
    "title": "Perturbation Analysis",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nThere are many flavours of automatic differenation (check JuliaDiff.org)\n \n \n\n\nForward Accumulation Mode\n\nisomorphic to dual number calculation\ncompute values and derivatives at the same time\nefficient for \\(f: R^n\\rightarrow R^m\\), with \\(n&lt;&lt;m\\)\n\n(keeps lots of empty gradients when \\(n&gt;&gt;m\\))\n\n\n\nReverse Accumulation Mode\n\nReverse Accumulation / Back Propagation\n\nefficient for \\(f: R^n\\rightarrow R^m\\), with \\(n&gt;&gt;m\\)\nrequires data storage (to keep intermediate values)\ngraph / example\n\nVery good for machine learning:\n\n\\(\\nabla_{\\theta} F(x;\\theta)\\) where \\(F\\) is a scalar objective"
  },
  {
    "objectID": "lectures/perturbation/index.html#libraries-for-autodiff",
    "href": "lectures/perturbation/index.html#libraries-for-autodiff",
    "title": "Perturbation Analysis",
    "section": "Libraries for AutoDiff",
    "text": "Libraries for AutoDiff\n\nSee JuliaDiff: http://www.juliadiff.org/\n\nForwardDiff.jl\nReverseDiff.jl\n\nZygote.jl\nDeep learning framework:\n\nhigher order diff w.r.t. any vector -&gt; tensor operations\nFlux.jl, MXNet.jl, Tensorflow.jl\n\n\n\n\n\n\n\n\n\nExample with ForwardDiff\n\n\nExample with ForwardDiff:\n  using ForwardDiff\nForwardDiff.jacobian(u-&gt;[u[1]^2, u[2]+u[1]], [0.1,0.2])"
  },
  {
    "objectID": "lectures/perturbation/index.html#our-problem",
    "href": "lectures/perturbation/index.html#our-problem",
    "title": "Perturbation Analysis",
    "section": "Our problem",
    "text": "Our problem\n\n\nGeneral formulation of a linearized model: \\[ \\begin{eqnarray} A s_t + B x_t + C s_{t+1} + D x_{t+1} & = & 0_{n_x} \\\\\ns_{t+1} & = & E s_t + F x_t \\end{eqnarray}\\] where:\n\n\\(s_t \\in \\mathbb{R}^{n_s}\\) is a vector of states\n\\(x_t \\in \\mathbb{R}^{n_x}\\) is a vector of controls\n\nRemark:\n\nfirst equation is forward looking\nsecond equation is backward looking\n\n\nIn the neoclassical model: \\[\\begin{eqnarray}\ns_t & = & (\\Delta z_t, \\Delta k_t) \\\\\nx_t & = & (\\Delta i_t, \\Delta c_t)\n\\end{eqnarray}\\]\nThe linearized system is: \\[\\begin{eqnarray}\nA & = & ...\\\\\nB & = & ...\\\\\nC & = & ...\\\\\nD & = & ...\\\\\nE & = & ...\\\\\nF & = &\n\\end{eqnarray}\\]"
  },
  {
    "objectID": "lectures/perturbation/index.html#solution",
    "href": "lectures/perturbation/index.html#solution",
    "title": "Perturbation Analysis",
    "section": "Solution",
    "text": "Solution\n\n\nWhat is the solution of our problem?\n\nAt date \\(t\\) controls must be chosen as a function of (predetermined) states\nMathematically speaking, the solution is a function \\(\\varphi\\) such that: \\[\\forall t, x_t = \\varphi(s_t)\\]\nSince the model is linear we look for un unknown matrix \\(X \\in \\mathbb{R}^{n_x} \\times \\mathbb{R}^{n_s}\\) such that: \\[\\Delta x_t = X \\Delta s_t\\]\n\n\nIn the neoclassical model\n\n\nThe states are \\(k_t\\) and \\(z_t\\)\nThe controls \\(i_t\\) and \\(c_t\\) must be a function of the states\n\nthere is a decision rule \\(i()\\), \\(c()\\) such that \\[i_t = i(z_t, k_t)\\] \\[c_t = c(z_t, k_t)\\]\n\nIn the linearized model: \\[\\Delta i_t =i_z \\Delta z_t + i_k \\Delta k_t\\] \\[\\Delta c_t =c_z \\Delta z_t + c_k \\Delta k_t\\]"
  },
  {
    "objectID": "lectures/perturbation/index.html#optimality-condition",
    "href": "lectures/perturbation/index.html#optimality-condition",
    "title": "Perturbation Analysis",
    "section": "Optimality condition:",
    "text": "Optimality condition:\n\n\nReplacing in the system: \n\\[\\Delta x_t  =  X \\Delta s_t\\] \\[\\Delta s_{t+1}  =  E \\Delta s_t + F X \\Delta s_t\\] \\[\\Delta x_{t+1}  =  X \\Delta s_{t+1}\\] \\[A \\Delta s_t + B \\Delta x_t + C \\Delta s_{t+1} + D \\Delta x_{t+1}  =  0\\]\nIf we make the full substitution:\n\\[( (A + B X) + ( D X + C) ( E  + F X ) ) \\Delta s_t = 0\\]\n\nThis must be true for all \\(s_t\\). We get the special Ricatti equation:\n\\[(A + B {\\color{red}{X}} ) + ( D {\\color{red}{X}} + C) ( E  + F {\\color{red}X} ) = 0\\]\nThis is a quadratic, matrix ( \\(X\\) is 2 by 2 ) equation:\n\nrequires special solution method\nthere are multiple solutions: which should we choose?\ntoday: linear time iteration selects only one solution\n\nalternative: eigenvalues analysis"
  },
  {
    "objectID": "lectures/perturbation/index.html#linear-time-iteration",
    "href": "lectures/perturbation/index.html#linear-time-iteration",
    "title": "Perturbation Analysis",
    "section": "Linear Time Iteration",
    "text": "Linear Time Iteration\n\nLet’s be more subtle: define\n\n\\(X\\): decision rule today and\n\\(\\tilde{X}\\): is decision rule tomorrow. \\[\\begin{eqnarray}\n\\Delta x_t & =&  X \\Delta s_t \\\\\n\\Delta s_{t+1} & = & E \\Delta  s_t + F X \\Delta s_t \\\\\n\\Delta x_{t+1} & = & \\tilde{X} \\Delta s_{t+1} \\\\\nA \\Delta s_t + B \\Delta x_t + C \\Delta s_{t+1} + D \\Delta x_{t+1} & = & 0\n\\end{eqnarray}\\]\n\nWe get, \\(\\forall s_t\\): \\[(A + B X) + (C + D \\tilde{X}) ( E  + F X ) ) \\Delta s_t = 0 \\]\nAgain, this must be zero in all states \\(\\Delta s_t\\)."
  },
  {
    "objectID": "lectures/perturbation/index.html#linear-time-iteration-2",
    "href": "lectures/perturbation/index.html#linear-time-iteration-2",
    "title": "Perturbation Analysis",
    "section": "Linear Time Iteration (2)",
    "text": "Linear Time Iteration (2)\n\n\n\nWe get the equation: \\[\\begin{eqnarray}\nF(X, \\tilde{X}) & = & (A + B X) + ( C+ D \\tilde{X}) ( E  + F X ) \\\\&=& 0\n\\end{eqnarray}\\]\nConsider the linear time iteration algorithm\nWhen the model is well-specified it is guaranteed to converge to the right solution.\n\ncf linear time iteration by Pontus Rendahl (link)\n\nThere are simple criteria to check that the solution is right, and that the model is well specified\n\\(T\\) is the time iteration operator… for linear models\n\nit does forward iteration (\\(X_t\\) as a function of \\(X_{t+1}\\))\n\n\n\nAlgorithm:\n\nchoose stopping criteria: \\(\\epsilon_0\\) and \\(\\eta_0\\)\nchoose random \\(X_0\\)\ngiven \\(X_n\\):\n\ncompute \\(X_{n+1}\\) such that \\(F(X_{n+1}, X_{n}) = 0\\) \\[(B + (C+D X_{n})F)X_{n+1} + A  + (C+D X_n )E=0\\]\\[X_{n+1} = - (B + (C + D X_n) F)^{-1} (A + (C+DX_n)E)\\]\\[X_{n+1} = T(X_n)\\]\ncompute:\n\n\\(\\eta_n = |X_{n+1} - X_n|\\)\n\\(\\epsilon_n = F(X_{n+1}, X_{n+1})\\)\n\nif \\(\\eta_n&lt;\\eta_0\\) and \\(\\epsilon_n&lt;\\epsilon_0\\)\n\nstop and return \\(X_{n+1}\\)\notherwise iterate with \\(X_{n+1}\\)"
  },
  {
    "objectID": "lectures/perturbation/index.html#simulating-the-model",
    "href": "lectures/perturbation/index.html#simulating-the-model",
    "title": "Perturbation Analysis",
    "section": "Simulating the model",
    "text": "Simulating the model\n\nSuppose we have found the solution \\(\\Delta x_t  = X \\Delta s_t\\)\nRecall the transition equation: \\(\\Delta s_{t+1} = F \\Delta s_t + G \\Delta x_t\\)\nWe can now compute the model evolution following initial deviation in the state: \\[\\Delta s_t = \\underbrace{(F + G X)}_{P} \\Delta s_{t-1}\\]\n\n\\(P\\) is the simulation operator\nit is a backward operator (TODO: example of a reaction to a shock)\n\nThe system is stable if the biggest eigenvalue of \\(P\\) is smaller than one…\n… or if its spectral radius is smaller than 1: \\[\\rho(P)&lt;1\\]\nThis condition is called backward stability\n\nit rules out explosive solutions\nif \\(\\rho(P)&gt;1\\) one can always find \\(s_0\\) such that the model simulation diverges"
  },
  {
    "objectID": "lectures/perturbation/index.html#spectral-radius",
    "href": "lectures/perturbation/index.html#spectral-radius",
    "title": "Perturbation Analysis",
    "section": "Spectral radius",
    "text": "Spectral radius\n\n\n\nHow do you compute the spectral radius of matrix P?\n\nnaive approach: compute all eigenvalues, check the value of the biggest one…\n\nusing LinearAlgebra\nM = rand(2,2)\nmaximum(abs, eigvals(M))\n\nbecomes limited when size of matrix growth\nanother approach: power iteration method\n\nPower iteration method:\n\nworks for matrices and linear operators\ntake a linear operator \\(L\\) over a Banach Space \\(\\mathcal{B}\\) (vector space with a norm)\nuse the fact that for most \\(u_0\\in \\mathcal{B}\\), \\(\\frac{|L^{n+1} u_0|}{|L^n u_0|}\\rightarrow \\rho(L)\\)\n\n\n\nAlgorithm:\n\nchoose tolerance criterium: \\(\\eta&gt;0\\)\nchoose random initial \\(x_0\\) and define \\(u_0 = \\frac{x_0}{|x_0|}\\)\n\nby construction: \\(|u_0|=1\\)\n\ngiven \\(u_n\\), compute\n\n\\(x_{n+1} = L.u_n\\)\n\\(u_{n+1} = \\frac{x_{n+1}}{|x_{n+1}|}\\)\ncompute \\(\\eta_{n+1} = |u_{n+1} - u_n|\\)\nif \\(\\eta_{n+1}&lt;\\eta\\):\n\nstop and return \\(|x_{n+1}|\\)\nelse iterate with \\(u_{n+1}\\)"
  },
  {
    "objectID": "lectures/perturbation/index.html#stability-of-the-backward-operator",
    "href": "lectures/perturbation/index.html#stability-of-the-backward-operator",
    "title": "Perturbation Analysis",
    "section": "Stability of the backward operator",
    "text": "Stability of the backward operator\nTo solve the model we use the backard operator: \\[T: \\begin{eqnarray} \\mathbb{R}^{n_x} \\times \\mathbb{R}^{n_s}  & \\rightarrow &  \\mathbb{R}^{n_x} \\times \\mathbb{R}^{n_s}  \\\\X_{t+1} & \\mapsto & X_t \\text{s.t.} F(X_t,X_{t+1})=0\\end{eqnarray}\\]\n\nWhat about its stability?\nRecall: fixed point \\(\\overline{z}\\) of recursive sequence \\(z_n=f(z_{n_1})\\) is stable if \\(|f^{\\prime}(\\overline{z})|&lt;1\\)\n\n\n\n\nWe need to study \\(T^{\\prime}\\) of (\\(X\\)).\n\nbut \\(T\\) maps a matrix to another matrix 🐉😓\nhow do we differentiate it?"
  },
  {
    "objectID": "lectures/perturbation/index.html#differentials",
    "href": "lectures/perturbation/index.html#differentials",
    "title": "Perturbation Analysis",
    "section": "Differentials",
    "text": "Differentials\n\nConsider a Banach Space \\(\\mathcal{B}\\).\nConsider an operator (i.e. a function): \\(\\mathcal{T}: \\mathcal{B} \\rightarrow \\mathcal{B}\\).\nConsider \\(\\overline{x} \\in \\mathcal{B}\\).\n\\(\\mathcal{T}\\) is differentiable at \\(\\overline{x}\\) if there exists a bounded linear operator \\(L \\in \\mathcal{L}(\\mathcal{B})\\) such that: \\[\\mathcal{T}(x) = \\overline{x} + L.(x-\\overline{x}) + o(|x-\\overline{x}|)\\]\n\nwhen it exists we denote this operator by \\(\\mathcal{T}^{\\prime}(\\overline{x})\\)\n\nRemarks:\n\nBounded operator means: \\(\\sup_{|x|=1} |L.x|&lt;+\\infty\\)\nThis definition of a derivative is usually referred to as Fréchet-derivative"
  },
  {
    "objectID": "lectures/perturbation/index.html#examples-of-linear-operators",
    "href": "lectures/perturbation/index.html#examples-of-linear-operators",
    "title": "Perturbation Analysis",
    "section": "Examples of linear operators",
    "text": "Examples of linear operators\n\n\\(x\\) vector, A a matrix, \\(T(x) = Ax\\)\n\nthen \\(T(x+u) = Ax + A.u + 0\\)\n\\(T^{\\prime}(x) = A\\) for all x\n\n\\(A\\) a matrix, B a matrix, X a matrix: \\(T(X) = A X B\\)\n\nthen \\(T(X+u) = A X B + A u B + 0\\)\n\\(T^{\\prime}(X).u = A u B\\)\n\n\\(A\\) a matrix, X a matrix: \\(T(X) = A X B\\)\n\nthen \\(T(X+u) = A X B + A u B\\)\n\\(T^{\\prime}(X).u = A u B\\)"
  },
  {
    "objectID": "lectures/perturbation/index.html#back-to-the-time-iteration-operator",
    "href": "lectures/perturbation/index.html#back-to-the-time-iteration-operator",
    "title": "Perturbation Analysis",
    "section": "Back to the time iteration operator",
    "text": "Back to the time iteration operator\n\n\\(T(X)\\) is implicitly defined by \\(F(T(X), X)=0\\)\n\\(F(X,Y) = (A + B X) + ( C+ D Y) ( E  + F X )\\)\n\nit is linear in \\(X\\) and in \\(Y\\)\n\n\\(F^{\\prime}_X (X, Y).u = (B + (C+DY)F) u\\)\n\na regular matrix multiplication\nits inverse is: \\(F^{\\prime}_X (X, Y)^{-1} = (B + (C+DY)F)^{-1}\\)\n\n\\(F^{\\prime}_Y (X, Y).u = D u (E+FX)\\)\n\na linear operation on matrices"
  },
  {
    "objectID": "lectures/perturbation/index.html#the-derivative-of-the-time-iteration-operator",
    "href": "lectures/perturbation/index.html#the-derivative-of-the-time-iteration-operator",
    "title": "Perturbation Analysis",
    "section": "The derivative of the time-iteration operator",
    "text": "The derivative of the time-iteration operator\n\nImplicit relation can be differentiated: \\[F^{\\prime}_X (T(X), X) T^{\\prime} (X) + F_Y^{\\prime}(T(X),X) = 0\\]\n\\(F^{\\prime}_X (T(X), X)\\) being a regular matrix, it is (conceptually) easy to invert: \\[T^{\\prime}(X) = -(F^{\\prime}_X (T(X), X))^{-1}F_Y^{\\prime}(T(X),X)\\]\nFinally, we get the explicit formula for the linear operator \\(T^{\\prime}\\) computed at the steady state: \\[T^{\\prime}(\\overline{X}).u = ((B + (C+D \\overline{X})F)^{-1})D u (E+F \\overline{X})\\]\nWe can compute the spectral radius of \\(T^{\\prime}(\\overline{X})\\) using the power iteration method"
  },
  {
    "objectID": "lectures/perturbation/index.html#recap",
    "href": "lectures/perturbation/index.html#recap",
    "title": "Perturbation Analysis",
    "section": "Recap",
    "text": "Recap\n\nWe compute the derivatives of the model\nTime iteration algorithm, starting from an initial guess \\(X_0\\) and we repeat until convergence: \\[X_{n+1} = (B + (C + D X_n) F)^{-1} (A + (C+DX_n)E)\\]\nWe compute the spectral radius of two operators to ensure the model is well defined and that the solution is the right one.\nbackward stability: derivative of simulation operator \\[\\boxed{\\rho(F + H \\overline{X} )}\\]\nforward stability: derivative of time iteration operator \\[\\boxed{\\rho \\left( u\\mapsto ((B + (C+D \\overline{X})F)^{-1})D u (E+F \\overline{X}) \\right)}\\]"
  },
  {
    "objectID": "scpo/index_2026.html",
    "href": "scpo/index_2026.html",
    "title": "econobits",
    "section": "",
    "text": "Work in Progress"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website is devoted to teaching computational economics."
  },
  {
    "objectID": "lectures/convergence/index.html#life-of-a-computational-economist",
    "href": "lectures/convergence/index.html#life-of-a-computational-economist",
    "title": "Convergence of Sequences (1)",
    "section": "Life of a computational economist",
    "text": "Life of a computational economist"
  },
  {
    "objectID": "lectures/convergence/index.html#life-of-a-computational-economist-1",
    "href": "lectures/convergence/index.html#life-of-a-computational-economist-1",
    "title": "Convergence of Sequences (1)",
    "section": "Life of a computational economist",
    "text": "Life of a computational economist\nVideo\n\n\nWe spend a lot of time waiting for algorithms to converge!\n\n\nsolution 1: program better\n\nsolution 2: better algorithms\n\neven better: understand convergence properties (information about the model)"
  },
  {
    "objectID": "lectures/convergence/index.html#recursive-sequence",
    "href": "lectures/convergence/index.html#recursive-sequence",
    "title": "Convergence of Sequences (1)",
    "section": "Recursive sequence",
    "text": "Recursive sequence\nConsider a function \\(f: R^n\\rightarrow R^n\\) and a recursive sequence \\((x_n)\\) defined by \\(x_0\\in R^n\\) and \\(x_n = f(x_{n-1})\\).\nWe want to compute a fixed point of \\(f\\) and study its properties.\n  \n\nToday: Some methods for the case \\(n=1\\).\n  \n\n\nAnother day:\n\nthe matrix case: \\(x_n = A^n x_0\\) whre \\(A\\in R^n \\times R^n\\)\nthe finite nonlinear case: \\(x_n = f(x_{n-1})\\)"
  },
  {
    "objectID": "lectures/convergence/index.html#example-growth-model",
    "href": "lectures/convergence/index.html#example-growth-model",
    "title": "Convergence of Sequences (1)",
    "section": "Example: growth model",
    "text": "Example: growth model\n\n\n\nSolow growth model:\n\ncapital accumulation: \\[k_t = (1-\\delta)k_{t-1} + i_{t-1}\\]\nproduction: \\[y_t = k_t^\\alpha\\]\nconsumption: \\[c_t = (1-{\\color{red}s})y_t\\] \\[i_t = s y_t\\]\n\n\n\n\n\n\n\nFor a given value of \\({\\color{red} s}\\in\\mathbb{R}^{+}\\) ( \\({\\color{red} s}\\) is a decision rule) \\[k_{t+1} = f(k_t, {\\color{red} s})\\]\n\nbackward-looking iterations\nSolow hypothesis: saving rate is invariant\n\n\n\nQuestions:\n\nWhat is the steady-state?\nCan we characterize the transition back the steady-state?\nCharacterize the dynamics close to the steady-state?\nwhat is the optimal \\(s\\) ?"
  },
  {
    "objectID": "lectures/convergence/index.html#another-example-linear-new-keynesian-model",
    "href": "lectures/convergence/index.html#another-example-linear-new-keynesian-model",
    "title": "Convergence of Sequences (1)",
    "section": "Another example: linear new keynesian model",
    "text": "Another example: linear new keynesian model\n\n\n\nBasic New Keynesian model (full derivation if curious )\n\nnew philips curve (PC):\\[\\pi_t = \\beta \\mathbb{E}_t \\pi_{t+1} + \\kappa y_t\\]\ndynamic investment-saving equation (IS):\\[y_t = \\beta \\mathbb{E}_t y_{t+1} - \\frac{1}{\\sigma}(i_t - \\mathbb{E}_t(\\pi_{t+1}) ) - {\\color{green} z_t}\\]\ninterest rate setting (taylor rule): \\[i_t = \\alpha_{\\pi} \\pi_t + \\alpha_{y} y_t\\]\n\n\nSolving the system:\n\nsolution: \\(\\begin{bmatrix}\\pi_t \\\\\\\\ y_t \\end{bmatrix} = {\\color{red} c} z_t\\)\n\n\n\n\n\nforward looking:\n\ntake \\(\\begin{bmatrix}\\pi_{t+1} \\\\\\\\ y_{t+1} \\end{bmatrix} = {\\color{red} {c_n}} z_{t+1}\\)\ndeduce \\(\\begin{bmatrix}\\pi_{t} \\\\\\\\ y_{t} \\end{bmatrix} = {\\color{red} {c_{n+1}}} z_{t}\\)\n\\(\\mathcal{T}: \\underbrace{c_{n}}_{t+1: \\; \\text{tomorrow}} \\rightarrow \\underbrace{c_{n+1}}_{t: \\text{today}}\\) is the time-iteration operator (a.k.a. Coleman operator)\n\n\n\n\n\n\nQuestions:\n\nWhat is the limit to \\(c_{t+1} = \\mathcal{T} c_n\\) ?\nUnder wich conditions (on \\(\\alpha_{\\pi}, \\alpha_y\\)) is it convergent ?\n\ndeterminacy conditions\ninterpretation: does the central bank manage to control inflation expectations?"
  },
  {
    "objectID": "lectures/convergence/index.html#recursive-sequence-2",
    "href": "lectures/convergence/index.html#recursive-sequence-2",
    "title": "Convergence of Sequences (1)",
    "section": "Recursive sequence (2)",
    "text": "Recursive sequence (2)\n\nWait: does a fixed point exist?\n\nwe’re not very concerned by the existence problem here\nwe’ll be happy with local conditions (existence, uniqueness) around a solution\n\nIn theoretical work, there are many fixed-point theorems to choose from.\nFor instance, we can assume there is an interval such that \\(f([a,b])\\subset[a,b]\\). Then we know there exists \\(x\\) in \\([a,b]\\) such that \\(f(x)=x\\). But there can be many such points."
  },
  {
    "objectID": "lectures/convergence/index.html#example-growth-model-with-multiple-fixed-points",
    "href": "lectures/convergence/index.html#example-growth-model-with-multiple-fixed-points",
    "title": "Convergence of Sequences (1)",
    "section": "Example: growth model with multiple fixed points",
    "text": "Example: growth model with multiple fixed points\n\nMultiple equilibria in the growth modelIn the growth model, if we change the production function: \\(y=k^{\\alpha}\\) for a nonconvex/nonmonotonic one, we can get multiple fixed points."
  },
  {
    "objectID": "lectures/convergence/index.html#convergence",
    "href": "lectures/convergence/index.html#convergence",
    "title": "Convergence of Sequences (1)",
    "section": "Convergence",
    "text": "Convergence\n\nGiven \\(f: R \\rightarrow R\\)\nHow do we characterize behaviour around \\(x\\) such that \\(f(x)=x\\)?\n\n\n\nStability criterium:\n\nif \\(|f^{\\prime}(x)|&gt;1\\): sequence is unstable and will not converge to \\(x\\) except by chance\nif \\(|f^{\\prime}(x)|&lt;1\\): \\(x\\) is a stable fixed point\nif \\(|f^{\\prime}(x)|=1\\): ??? (look at higher order terms, details ↓)"
  },
  {
    "objectID": "lectures/convergence/index.html#section",
    "href": "lectures/convergence/index.html#section",
    "title": "Convergence of Sequences (1)",
    "section": "",
    "text": "To get the intution about local convergence assume, you have an initial point \\(x_n\\) close to the steady state and consider the following expresion:\n\\(x_{n+1} - x = f(x_n) - f(x) = f^{\\prime}(x) (x_n-x) + o( (x_n-x) )\\)\nIf one sets aside the error term (which one can do with full mathematical rigour), the dynamics for very small perturbations are given by:\n\\(|x_{n+1} - x| = |f^{\\prime}(x)| |x_n-x|\\)\nWhen \\(|f^{\\prime}(x)|&lt;1\\), the distance to the target decreases at each iteration and we have convergence. When \\(|f^{\\prime}(x)|&gt;1\\) there is local divergence."
  },
  {
    "objectID": "lectures/convergence/index.html#section-1",
    "href": "lectures/convergence/index.html#section-1",
    "title": "Convergence of Sequences (1)",
    "section": "",
    "text": "What about the case \\(|f^{\\prime}(x)=1|\\)? Many cases are possible. To distinguish between them, one needs to inspect higher order derivatives.\n\nwhen \\(|f^{\\prime}(x)=1|\\), \\(|f^{\\prime\\prime}(x)|\\neq 0\\) the series will convergence, only if \\((x_0-x)f^{\\prime\\prime}(x)&lt;0\\), i.e. starting from one side of the fixed point. The steady-state is not stable.\nWhen \\(|f^{\\prime}(x)=1|\\), \\(|f^{\\prime\\prime}(x)| = 0\\), \\(|f^{\\prime \\prime\\prime}(x)|\\neq 0\\) the series will converge, only if \\(f^{\\prime}(x)(f^{\\prime\\prime\\prime}(x))&lt;1\\)\n\nIn general, there is stability only if the function \\(f\\) is crossing the 45 degrees line (when \\(f^ {\\prime}(x)=1)\\), or the -45 degrees line (when \\(f^ {\\prime}(x)=1\\))\nMathematically, this involves, that:\n\nthe first non-zero coefficient \\(f^{k}(x)\\) with \\(k&gt;1\\) has odd order (\\(k\\) odd)\nit has the right sign"
  },
  {
    "objectID": "lectures/convergence/index.html#change-the-problem",
    "href": "lectures/convergence/index.html#change-the-problem",
    "title": "Convergence of Sequences (1)",
    "section": "Change the problem",
    "text": "Change the problem\n\nSometimes, we are interested in tweaking the convergence speed: \\[x_{n+1} = (1-\\lambda) x_n + \\lambda f(x_n)\\]\n\n\\(\\lambda\\) is the learning rate:\n\n\\(\\lambda&gt;1\\): acceleration\n\\(\\lambda&lt;1\\): dampening\n\n\n\n\n\nWe can also replace the function by another one \\(g\\) such that \\(g(x)=x\\iff f(x)=x\\), for instance: \\[g(x)=x-\\frac{f(x)-x}{f^{\\prime}(x)-1}\\]\n\nthis is the Newton iteration"
  },
  {
    "objectID": "lectures/convergence/index.html#dynamics-around-a-stable-point",
    "href": "lectures/convergence/index.html#dynamics-around-a-stable-point",
    "title": "Convergence of Sequences (1)",
    "section": "Dynamics around a stable point",
    "text": "Dynamics around a stable point\n\nWe can write successive approximation errors: \\[|x_t - x_{t-1}| =  | f(x_{t-1}) - f(x_{t-2})| \\] \\[|x_t - x_{t-1}| \\sim |f^{\\prime}(x_{t-1})| |x_{t-1} - x_{t-2}| \\]\nRatio of successive approximation errors \\[\\lambda_t =  \\frac{ |x_{t} - x_{t-1}| } { |x_{t-1} - x_{t-2}|}\\]\nWhen the sequence converges: \\[\\lambda_t \\rightarrow | f^{\\prime}(\\overline{x}) |\\]"
  },
  {
    "objectID": "lectures/convergence/index.html#dynamics-around-a-stable-point-2",
    "href": "lectures/convergence/index.html#dynamics-around-a-stable-point-2",
    "title": "Convergence of Sequences (1)",
    "section": "Dynamics around a stable point (2)",
    "text": "Dynamics around a stable point (2)\nHow do we derive an error bound? Suppose that we have \\(\\overline{\\lambda}&gt;|f^{\\prime}(x_k)|\\) for all \\(k\\geq k_0\\):\n\\[|x_t - x| \\leq |x_t - x_{t+1}| + |x_{t+1} - x_{t+2}| + |x_{t+2} - x_{t+3}| + ... \\]\n\\[|x_t - x| \\leq |x_t - x_{t+1}| + |f(x_{t}) - f(x_{t+1})| + |f(x_{t+1}) - f(x_{t+2})| + ... \\]\n\\[|x_t - x| \\leq |x_t - x_{t+1}| + \\overline{\\lambda} |x_t - x_{t+1}| + \\overline{\\lambda}^2 |x_t - x_{t+1}| + ... \\]\n\\[|x_t - x| \\leq \\frac{1} {1-\\overline{\\lambda}} | x_t - x_{t+1} |\\]"
  },
  {
    "objectID": "lectures/convergence/index.html#how-do-we-improve-convergence",
    "href": "lectures/convergence/index.html#how-do-we-improve-convergence",
    "title": "Convergence of Sequences (1)",
    "section": "How do we improve convergence ?",
    "text": "How do we improve convergence ?\n\\[\\frac{|x_{t-1} - x_{t-2}|} {|x_t - x_{t-1}|} \\sim |f^{\\prime}(x_{t-1})|  \\]\ncorresponds to the case of linear convergence (kind of slow)."
  },
  {
    "objectID": "lectures/convergence/index.html#aitkens-extrapolation",
    "href": "lectures/convergence/index.html#aitkens-extrapolation",
    "title": "Convergence of Sequences (1)",
    "section": "Aitken’s extrapolation",
    "text": "Aitken’s extrapolation\nWhen convergence is geometric, we have: \\[ \\lim_{x\\rightarrow \\infty}\\frac{ x_{t+1}-x}{x_t-x} = \\lambda \\in \\mathbb{R}^{\\star}\\]\nWhich implies:\n\\[\\frac{ x_{t+1}-x}{x_t-x} \\sim \\frac{ x_{t}-x}{x_{t-1}-x}\\]"
  },
  {
    "objectID": "lectures/convergence/index.html#aitkens-extrapolation-2",
    "href": "lectures/convergence/index.html#aitkens-extrapolation-2",
    "title": "Convergence of Sequences (1)",
    "section": "Aitken’s extrapolation (2)",
    "text": "Aitken’s extrapolation (2)\nTake \\(x_{t-1}, x_t\\) and \\(x_{t+1}\\) as given and solve for \\(x\\):\n\\[x = \\frac{x_{t+1}x_{t-1} - x_{t}^2}{x_{t+1}-2x_{t} + x_{t-1}}\\]\nor after some reordering\n\\[x = x_{t-1} - \\frac{(x_t-x_{t-1})^2}{x_{t+1}-2 x_t + x_{t-1}}\\]"
  },
  {
    "objectID": "lectures/convergence/index.html#steffensens-method",
    "href": "lectures/convergence/index.html#steffensens-method",
    "title": "Convergence of Sequences (1)",
    "section": "Steffensen’s Method:",
    "text": "Steffensen’s Method:\n\nstart with a guess \\(x_0\\), compute \\(x_1=f(x_0)\\) and \\(x_2=f(x_1)\\)\nuse Aitken’s guess for \\(x^{\\star}\\). If required tolerance is met, stop.\notherwise, set \\(x_0 = x^{\\star}\\) and go back to step 1.\n\nIt can be shown that the sequence generated from Steffensen’s method converges quadratically, that is\n\\(\\lim_{t\\rightarrow\\infty} \\frac{x_{t+1}-x_t}{(x_t-x_{t-1})^2} \\leq M  \\in \\mathbb{R}^{\\star}\\)"
  },
  {
    "objectID": "lectures/convergence/index.html#convergence-speed",
    "href": "lectures/convergence/index.html#convergence-speed",
    "title": "Convergence of Sequences (1)",
    "section": "Convergence speed",
    "text": "Convergence speed\nRate of convergence of series \\(x_t\\) towards \\(x^{\\star}\\) is:\n\nlinear: \\[{\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x^{\\star}|}{|x_{t}-x^{\\star}|} = \\mu \\in R^+\\]\nsuperlinear: \\[{\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x^{\\star}|}{|x_{t}-x^{\\star}|} = 0\\]\nquadratic: \\[{\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x^{\\star}|}{|x_{t}-x^{\\star}|^{\\color{red}2}} = \\mu \\in R^+\\]"
  },
  {
    "objectID": "lectures/convergence/index.html#convergence-speed-1",
    "href": "lectures/convergence/index.html#convergence-speed-1",
    "title": "Convergence of Sequences (1)",
    "section": "Convergence speed",
    "text": "Convergence speed\nRemark: in the case of linear convergence:\n\\[{\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x_t|}{|x_{t}-x_{t-1}|} = \\mu \\in R^+ \\iff {\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x^{\\star}|}{|x_{t}-x^{\\star}|}=\\frac{1}{1-\\mu}\\]"
  },
  {
    "objectID": "lectures/convergence/index.html#in-practice",
    "href": "lectures/convergence/index.html#in-practice",
    "title": "Convergence of Sequences (1)",
    "section": "In practice",
    "text": "In practice\n\nProblem: Suppose one is trying to find \\(x\\) solving the model \\(G(x)=0\\)\n\nAn iterative algorithm provides a function \\(f\\) defining a recursive series \\(x_{t+1}\\).\n\nThe best practice consists in monitoring at the same time:\n\nthe success criterion: \\[\\epsilon_n = |G(x_n)|\\]\n\n\nhave you found the solution?\n\nthe successive approximation errors \\[\\eta_n = |x_{n+1} - x_n|\\]\n\n\nare you making progress?\n\nthe ratio of successive approximation errors \\[\\lambda_n = \\frac{\\eta_n}{\\eta_{n-1}}\\]\n\n\nwhat kind of convergence? (if \\(|\\lambda_n|&lt;1\\): OK, otherwise: ❓)"
  },
  {
    "objectID": "lectures/optimization/index.html#introduction-1",
    "href": "lectures/optimization/index.html#introduction-1",
    "title": "Optimization",
    "section": "Introduction",
    "text": "Introduction\nOptimization is everywhere in economics:\n\nto model agent’s behaviour: what would a rational agent do?\n\nconsumer maximizes utility from consumption\nfirm maximizes profit\n\nan economist tries to solve a model:\n\nfind prices that clear the market"
  },
  {
    "objectID": "lectures/optimization/index.html#two-kinds-of-optimization-problem",
    "href": "lectures/optimization/index.html#two-kinds-of-optimization-problem",
    "title": "Optimization",
    "section": "Two kinds of optimization problem:",
    "text": "Two kinds of optimization problem:\n\nroot finding: \\(\\text{find  $x$ in $X$ such that $f(x)=0$}\\)\nminimization/maximization \\(\\min_{x\\in X} f(x)\\) or \\(\\max_{x\\in X} f(x)\\)\noften a minimization problem can be reformulated as a root-finding problem\n\\[x_0 = {argmin}_{x\\in X} f(x) \\overbrace{\\iff}^{??} f^{\\prime} (x_0) = 0\\]"
  },
  {
    "objectID": "lectures/optimization/index.html#plan",
    "href": "lectures/optimization/index.html#plan",
    "title": "Optimization",
    "section": "Plan",
    "text": "Plan\n\ngeneral consideration about optimization problems\none-dimensional root-finding\none-dimensional optimization\nlocal root-finding\nlocal optimization\nconstrained optimization\nconstrained root-finding"
  },
  {
    "objectID": "lectures/optimization/index.html#optimization-tasks-come-in-many-flavours",
    "href": "lectures/optimization/index.html#optimization-tasks-come-in-many-flavours",
    "title": "Optimization",
    "section": "Optimization tasks come in many flavours",
    "text": "Optimization tasks come in many flavours\n\ncontinuous versus discrete optimization\nconstrained and unconstrained optimization\nglobal and local\nstochastic and deterministic optimization\nconvexity"
  },
  {
    "objectID": "lectures/optimization/index.html#continuous-versus-discrete-optimization",
    "href": "lectures/optimization/index.html#continuous-versus-discrete-optimization",
    "title": "Optimization",
    "section": "Continuous versus discrete optimization",
    "text": "Continuous versus discrete optimization\n\nChoice is picked from a given set (\\(x\\in X\\)) which can be:\n\ncontinuous: choose amount of debt \\(b_t \\in [0,\\overline{b}]\\), of capital \\(k_t \\in R^{+}\\)\ndiscrete: choose whether to repay or default \\(\\delta\\in{0,1}\\), how many machines to buy (\\(\\in N\\)), at which age to retire…\na combination of both: mixed integer programming"
  },
  {
    "objectID": "lectures/optimization/index.html#continuous-versus-discrete-optimization-2",
    "href": "lectures/optimization/index.html#continuous-versus-discrete-optimization-2",
    "title": "Optimization",
    "section": "Continuous versus discrete optimization (2)",
    "text": "Continuous versus discrete optimization (2)\n\nDiscrete optimization requires a lot of combinatorial thinking\n\nWe don’t cover it today.\n…if needed, we just test all choices until we find the best one\n\nSometimes a discrete choice can be approximated by a mixed strategy (i.e. a random strategy).\n\nInstead of \\(\\delta\\in{0,1}\\) we choose \\(x\\) in \\(prob(\\delta=1)=\\sigma(x)\\)\nwith \\(\\sigma(x)=\\frac{2}{1+\\exp(-x)}\\)"
  },
  {
    "objectID": "lectures/optimization/index.html#constrained-and-unconstrained-optimization",
    "href": "lectures/optimization/index.html#constrained-and-unconstrained-optimization",
    "title": "Optimization",
    "section": "Constrained and Unconstrained optimization",
    "text": "Constrained and Unconstrained optimization\n\nUnconstrained optimization: \\(x\\in R\\)\nConstrained optimization: \\(x\\in X\\)\n\nbudget set: \\(p_1 c_1 + p_2 c_2 \\leq I\\)\npositivity of consumption: \\(c \\geq 0\\).\n\nIn good cases, the optimization set is convex…\n\npretty much always in this course"
  },
  {
    "objectID": "lectures/optimization/index.html#stochastic-vs-determinstic",
    "href": "lectures/optimization/index.html#stochastic-vs-determinstic",
    "title": "Optimization",
    "section": "Stochastic vs Determinstic",
    "text": "Stochastic vs Determinstic\n\nCommon case, especially in machine learning \\[f(x) = E_{\\epsilon}[ \\xi (\\epsilon, x)]\\]\nOne wants to maximize (resp solve) w.r.t. \\(x\\) but it is costly to compute expectation precisely using Monte-Carlo draws (there are other methods).\nA stochastic optimization method allows to use noisy estimates of the expectation, and will still converge in expectation.\nFor now we focus on deterministic methods. Maybe later…"
  },
  {
    "objectID": "lectures/optimization/index.html#local-vs-global-algorithms",
    "href": "lectures/optimization/index.html#local-vs-global-algorithms",
    "title": "Optimization",
    "section": "Local vs global Algorithms",
    "text": "Local vs global Algorithms\n\nIn principle, there can be many roots (resp maxima) within the optimization set.\nAlgorithms that find them all are called “global”. For instance:\n\ngrid search\nsimulated annealing\n\nWe will deal only with local algorithms, and consider local convergence properties.\n\n-&gt;then it might work or not\nto perform global optimization just restart from different points."
  },
  {
    "objectID": "lectures/optimization/index.html#math-vs-practice",
    "href": "lectures/optimization/index.html#math-vs-practice",
    "title": "Optimization",
    "section": "Math vs practice",
    "text": "Math vs practice\n\nThe full mathematical treatment will typically assume that \\(f\\) is smooth (\\(\\mathcal{C}_1\\) or \\(\\mathcal{C}_2\\) depending on the algorithm).\nIn practice we often don’t know about these properties\n\nwe still try and check thqt we have a local optimal\n\nSo: fingers crossed"
  },
  {
    "objectID": "lectures/optimization/index.html#math-vs-practice-1",
    "href": "lectures/optimization/index.html#math-vs-practice-1",
    "title": "Optimization",
    "section": "Math vs practice",
    "text": "Math vs practice\nHere is the surface representing the objective that a deep neural network training algorithm tries to minimize.\n\nAnd yet, neural networks do great things!"
  },
  {
    "objectID": "lectures/optimization/index.html#what-do-you-need-to-know",
    "href": "lectures/optimization/index.html#what-do-you-need-to-know",
    "title": "Optimization",
    "section": "What do you need to know?",
    "text": "What do you need to know?\n\nbe able to handcode simple algos (Newton, Gradient Descent)\nunderstand the general principle of the various algorithms to compare them in terms of\n\nrobustness\nefficiency\naccuracy\n\nthen you can just switch the various options, when you use a library…"
  },
  {
    "objectID": "lectures/optimization/index.html#bisection",
    "href": "lectures/optimization/index.html#bisection",
    "title": "Optimization",
    "section": "Bisection",
    "text": "Bisection\n\nFind \\(x \\in [a,b]\\) such that \\(f(x) = 0\\). Assume \\(f(a)f(b) &lt;0\\).\nAlgorithm\n\nStart with \\(a_n, b_n\\). Set \\(c_n=(a_n+b_n)/2\\)\nCompute \\(f(c_n)\\)\n\n\nif \\(f(c_n)f(a_n)&lt;0\\) then set \\((a_{n+1},b_{n+1})=(a_n,c_n)\\)\nelse set \\((a_{n+1},b_{n+1})=(c_n,b_n)\\)\n\n\nIf \\(|f(c_n)|&lt;\\epsilon\\) and/or \\(\\frac{b-a}{2^n}&lt;\\delta\\) stop. Otherwise go back to 1."
  },
  {
    "objectID": "lectures/optimization/index.html#bisection-2",
    "href": "lectures/optimization/index.html#bisection-2",
    "title": "Optimization",
    "section": "Bisection (2)",
    "text": "Bisection (2)\n\nNo need for initial guess: globally convergent algorithm\n\nnot a global algorithm…\n… in the sense that it doesn’t find all solutions\n\n\\(\\delta\\) is a guaranteed accuracy on \\(x\\)\n\\(\\epsilon\\) is a measure of how good the solution is\nthink about your tradeoff: (\\(\\delta\\) or \\(\\epsilon\\) ?)"
  },
  {
    "objectID": "lectures/optimization/index.html#newton-algorithm",
    "href": "lectures/optimization/index.html#newton-algorithm",
    "title": "Optimization",
    "section": "Newton algorithm",
    "text": "Newton algorithm\n\nFind \\(x\\) such that \\(f(x) = 0\\). Use \\(x_0\\) as initial guess.\n\\(f\\) must be \\(\\mathcal{C_1}\\) and we assume we can compute its derivative \\(f^{\\prime}\\)\nGeneral idea:\n\nobserve that the zero \\(x^{\\star}\\) must satisfy \\[f(x^{\\star})=0=f(x_0)+f^{\\prime}(x_0)(x^{\\star}-x_0) + o(x-x_0)\\]\nHence a good approximation should be \\[x^{\\star}\\approx = x_0- f(x_0)/f^{\\prime}(x_0)\\]\nCheck it is good. otherwise, replace \\(x_0\\) by \\(x^{\\star}\\)"
  },
  {
    "objectID": "lectures/optimization/index.html#newton-algorithm-2",
    "href": "lectures/optimization/index.html#newton-algorithm-2",
    "title": "Optimization",
    "section": "Newton algorithm (2)",
    "text": "Newton algorithm (2)\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n- \\frac{f(x_n)}{f^{\\prime}(x_n)}=f^{\\text{newton}}(x_n)\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f(x_n)| &lt; \\epsilon\\)\n\nConvergence: quadratic"
  },
  {
    "objectID": "lectures/optimization/index.html#quasi-newton",
    "href": "lectures/optimization/index.html#quasi-newton",
    "title": "Optimization",
    "section": "Quasi-Newton",
    "text": "Quasi-Newton\n\nWhat if we can’t compute \\(f^{\\prime}\\) or it is expensive to do so?\n\nIdea: try to approximate \\(f^{\\prime}(x_n)\\) from the last iterates\n\nSecant method: \\[f^{\\prime}(x_n)\\approx \\frac{f(x_n)-f(x_{n-1})}{x_n-x_{n-1}}\\] \\[x_{n+1} = x_n- f(x_n)\\frac{x_n-x_{n-1}}{f(x_n)-f(x_{n-1})}\\]\n\nrequires two initial guesses: \\(x_1\\) and \\(x_0\\)\nsuperlinear convergence: \\(\\lim \\frac{x_t-x^{\\star}}{x_{t-1}-x^{\\star}}\\rightarrow 0\\)"
  },
  {
    "objectID": "lectures/optimization/index.html#limits-of-newtons-method",
    "href": "lectures/optimization/index.html#limits-of-newtons-method",
    "title": "Optimization",
    "section": "Limits of Newton’s method",
    "text": "Limits of Newton’s method\n\nHow could Newton method fail?\n\nbad guess\n\n-&gt; start with a better guess\n\novershoot\n\n-&gt; dampen the update (problem: much slower)\n-&gt; backtrack\n\nstationary point\n\n-&gt; if root of multiplicity \\(m\\) try \\(x_{n+1} = x_n- m \\frac{f(x_n)}{f^{\\prime}(x_n)}\\)"
  },
  {
    "objectID": "lectures/optimization/index.html#backtracking",
    "href": "lectures/optimization/index.html#backtracking",
    "title": "Optimization",
    "section": "Backtracking",
    "text": "Backtracking\n\nSimple idea:\n\nat stage \\(n\\) given \\(f(x_n)\\) compute Newton step \\(\\Delta_n=-\\frac{f(x_n)}{f^{\\prime}(x_n)}\\)\nfind the smallest \\(k\\) such that \\(|f(x_n-\\Delta/2^k)|&lt;|f(x_n)|\\)\nset \\(x_{n+1}=x_n-\\Delta/2^k\\)"
  },
  {
    "objectID": "lectures/optimization/index.html#golden-section-search",
    "href": "lectures/optimization/index.html#golden-section-search",
    "title": "Optimization",
    "section": "Golden section search",
    "text": "Golden section search\n\nMinimize \\(f(x)\\) for \\(x \\in [a,b]\\)\nChoose \\(\\Phi \\in [0,0.5]\\)\nAlgorithm:\n\nstart with \\(a_n &lt; b_n\\) (initially equal to \\(a\\) and \\(b\\))\ndefine \\(c_n = a_n+\\Phi(b_n-a_n)\\) and \\(d_n = a_n+(1-\\Phi)(b_n-a_n)\\)\n\nif \\(f(c_n)&lt;f(d_n)\\) set \\(a_{n+1},b_{n+1}=a_n, d_n\\)\nelse set \\(a_{n+1}, b_{n+1}= c_n, b_n\\)"
  },
  {
    "objectID": "lectures/optimization/index.html#golden-section-search-2",
    "href": "lectures/optimization/index.html#golden-section-search-2",
    "title": "Optimization",
    "section": "Golden section search (2)",
    "text": "Golden section search (2)\n\nThis is guaranteed to converge to a local minimum\nIn each step, the size of the interval is reduced by a factor \\(\\Phi\\)\nBy choosing \\(\\Phi=\\frac{\\sqrt{5}-1}{2}\\) one can save one evaluation by iteration.\n\nyou can check that either \\(c_{n+1} = d_n\\) or \\(d_{n+1} = c_n\\)\n\nRemark that bisection is not enough"
  },
  {
    "objectID": "lectures/optimization/index.html#gradient-descent",
    "href": "lectures/optimization/index.html#gradient-descent",
    "title": "Optimization",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nMinimize \\(f(x)\\) for \\(x \\in R\\) given initial guess \\(x_0\\)\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n (1-\\lambda)- \\lambda f^{\\prime}(x_n)\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f^{\\prime}(x_n)| &lt; \\epsilon\\)"
  },
  {
    "objectID": "lectures/optimization/index.html#gradient-descent-2",
    "href": "lectures/optimization/index.html#gradient-descent-2",
    "title": "Optimization",
    "section": "Gradient Descent (2)",
    "text": "Gradient Descent (2)\n\nUses local information\n\none needs to compute the gradient\nnote that gradient at \\(x_n\\) does not provide a better guess for the minimum than \\(x_n\\) itself\nlearning speed is crucial\n\nConvergence speed: linear\n\nrate depend on the learning speed\noptimal learning speed? the fastest for which there is convergence"
  },
  {
    "objectID": "lectures/optimization/index.html#newton-raphson-method",
    "href": "lectures/optimization/index.html#newton-raphson-method",
    "title": "Optimization",
    "section": "Newton-Raphson method",
    "text": "Newton-Raphson method\n\nMinimize \\(f(x)\\) for \\(x \\in R\\) given initial guess \\(x_0\\)\nBuild a local model of \\(f\\) around \\(x_0\\) \\[f(x) = f(x_0) + f^{\\prime}(x_0)(x-x_0) + f^{\\prime\\prime}(x_0)\\frac{(x-x_0)^2}{2} + o(x-x_0)^2\\]\nAccording to this model, \\[ f(x{\\star}) = min_x f(x)\\iff \\frac{d}{d x} \\left[ f(x_0) + f^{\\prime}(x_0)(x-x_0) + f^{\\prime\\prime}(x_0)\\frac{(x-x_0)^2}{2} \\right] = 0\\] which yields: \\(x^{\\star} = x_0 - \\frac{f^{\\prime}(x_0)}{f^{\\prime\\prime}(x_0)}\\)\nthis is Newton applied to \\(f^{\\prime}(x)=0\\)"
  },
  {
    "objectID": "lectures/optimization/index.html#newton-raphson-algorithm-2",
    "href": "lectures/optimization/index.html#newton-raphson-algorithm-2",
    "title": "Optimization",
    "section": "Newton-Raphson Algorithm (2)",
    "text": "Newton-Raphson Algorithm (2)\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n-\\frac{f^{\\prime}(x_0)}{f^{\\prime\\prime}(x_0)}\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f^{\\prime}(x_n)| &lt; \\epsilon\\)\n\nConvergence: quadratic"
  },
  {
    "objectID": "lectures/optimization/index.html#unconstrained-problems",
    "href": "lectures/optimization/index.html#unconstrained-problems",
    "title": "Optimization",
    "section": "Unconstrained problems",
    "text": "Unconstrained problems\n\nMinimize \\(f(x)\\) for \\(x \\in R^n\\) given initial guess \\(x_0 \\in R^n\\)\nMany intuitions from the 1d case, still apply\n\nreplace derivatives by gradient, jacobian and hessian\nrecall that matrix multiplication is not commutative\n\nSome specific problems:\n\nupdate speed can be specific to each dimension\nsaddle-point issues (for minimization)"
  },
  {
    "objectID": "lectures/optimization/index.html#quick-terminology",
    "href": "lectures/optimization/index.html#quick-terminology",
    "title": "Optimization",
    "section": "Quick terminology",
    "text": "Quick terminology\nFunction \\(f: R^p \\rightarrow R^q\\)\n\nJacobian: \\(J(x)\\) or \\(f^{\\prime}_x(x)\\), \\(p\\times q\\) matrix such that: \\[J(x)_{ij} = \\frac{\\partial f(x)_i}{\\partial x_j}\\]\nGradient: \\(\\nabla f(x) = J(x)\\), gradient when \\(q=1\\)\nHessian: denoted by \\(H(x)\\) or \\(f^{\\prime\\prime}_{xx}(x)\\) when \\(q=1\\): \\[H(x)_{jk} = \\frac{\\partial f(x)}{\\partial x_j\\partial x_k}\\]\nIn the following explanations, \\(|x|\\) denotes the supremum norm, but most of the following explanations also work with other norms."
  },
  {
    "objectID": "lectures/optimization/index.html#multidimensional-newton-raphson",
    "href": "lectures/optimization/index.html#multidimensional-newton-raphson",
    "title": "Optimization",
    "section": "Multidimensional Newton-Raphson",
    "text": "Multidimensional Newton-Raphson\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n- J(x_{n})^{-1}f(x_n)=f^{\\text{newton}}(x_n)\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f(x_n)| &lt; \\epsilon\\)\n\nConvergence: quadratic"
  },
  {
    "objectID": "lectures/optimization/index.html#multidimensional-newton-root-finding-2",
    "href": "lectures/optimization/index.html#multidimensional-newton-root-finding-2",
    "title": "Optimization",
    "section": "Multidimensional Newton root-finding (2)",
    "text": "Multidimensional Newton root-finding (2)\n\nwhat matters is the computation of the step \\(\\Delta_n = {\\color{\\red}{J(x_{n})^{-1}}} f(x_n)\\)\ndon’t compute \\(J(x_n)^{-1}\\)\n\nit takes less operations to compute \\(X\\) in \\(AX=Y\\) than \\(A^{-1}\\) then \\(A^{-1}Y\\)\nin Julia: X = A \\ Y\n\nstrategies to improve convergence:\n\ndampening: \\(x_n = (1-\\lambda)x_{n-1} - \\lambda \\Delta_n\\)\nbacktracking: choose \\(k\\) such that \\(|f(x_n-2^{-k}\\Delta_n)|\\)&lt;\\(|f(x_{n-1})|\\)\nlinesearch: choose \\(\\lambda\\in[0,1]\\) so that \\(|f(x_n-\\lambda\\Delta_n)|\\) is minimal"
  },
  {
    "objectID": "lectures/optimization/index.html#multidimensional-gradient-descent",
    "href": "lectures/optimization/index.html#multidimensional-gradient-descent",
    "title": "Optimization",
    "section": "Multidimensional Gradient Descent",
    "text": "Multidimensional Gradient Descent\n\nMinimize \\(f(x) \\in R\\) for \\(x \\in R^n\\) given \\(x_0 \\in R^n\\)\nAlgorithm\n\nstart with \\(x_n\\) \\[x_{n+1} = (1-\\lambda) x_n - \\lambda \\nabla f(x_n)\\]\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f(x_n)| &lt; \\epsilon\\)\n\nComments:\n\nlots of variants\nautomatic differentiation software makes gradient easy to compute\nconvergence is typically linear"
  },
  {
    "objectID": "lectures/optimization/index.html#gradient-descent-variants",
    "href": "lectures/optimization/index.html#gradient-descent-variants",
    "title": "Optimization",
    "section": "Gradient descent variants",
    "text": "Gradient descent variants"
  },
  {
    "objectID": "lectures/optimization/index.html#multidimensional-newton-minimization",
    "href": "lectures/optimization/index.html#multidimensional-newton-minimization",
    "title": "Optimization",
    "section": "Multidimensional Newton Minimization",
    "text": "Multidimensional Newton Minimization\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n-{\\color{\\red}{H(x_{n})^{-1}}}\\color{\\green}{ J(x_n)'}\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f(x_n)| &lt; \\epsilon\\)\n\nConvergence: quadratic\nProblem:\n\n\\(H(x_{n})\\) hard to compute efficiently\nrather unstable"
  },
  {
    "objectID": "lectures/optimization/index.html#quasi-newton-method-for-multidimensional-minimization",
    "href": "lectures/optimization/index.html#quasi-newton-method-for-multidimensional-minimization",
    "title": "Optimization",
    "section": "Quasi-Newton method for multidimensional minimization",
    "text": "Quasi-Newton method for multidimensional minimization\n\nRecall the secant method:\n\n\\(f(x_{n-1})\\) and \\(f(x_{n-2})\\) are used to approximate \\(f^{\\prime}(x_{n-2})\\).\nIntuitively, \\(n\\) iterates would be needed to approximate a hessian of size \\(n\\)….\n\nBroyden method: takes \\(2 n\\) steps to solve a linear problem of size \\(n\\)\n\nuses past information incrementally"
  },
  {
    "objectID": "lectures/optimization/index.html#quasi-newton-method-for-multidimensional-minimization-1",
    "href": "lectures/optimization/index.html#quasi-newton-method-for-multidimensional-minimization-1",
    "title": "Optimization",
    "section": "Quasi-Newton method for multidimensional minimization",
    "text": "Quasi-Newton method for multidimensional minimization\n\nConsider the approximation: \\[f(x_n)-f(x_{n-1}) \\approx J(x_n) (x_n - x_{n-1})\\]\n\n\\(J(x_n)\\) is unknown and cannot be determined directly as in the secant method.\nidea: \\(J(x_n)\\) as close as possible to \\(J(x_{n-1})\\) while solving the secant equation\nformula: \\[J_n = J_{n-1} + \\frac{(f(x_n)-f(x_{n-1})) - J_{n-1}(x_n-x_{n-1})}{||x_n-x_{n-1}||^2}(x_n-x_{n-1})^{\\prime}\\]"
  },
  {
    "objectID": "lectures/optimization/index.html#gauss-newton-minimization",
    "href": "lectures/optimization/index.html#gauss-newton-minimization",
    "title": "Optimization",
    "section": "Gauss-Newton Minimization",
    "text": "Gauss-Newton Minimization\n\nRestrict to least-square minimization: \\(min_x \\sum_i f(x)_i^2 \\in R\\)\nThen up to first order, \\(H(x_n)\\approx J(x_n)^{\\prime}J(x_n)\\)\nUse the step: \\(({J(x_n)^{\\prime}J(x_n)})^{-1}\\color{\\green}{ J(x_n)}\\)\nConvergence:\n\ncan be quadratic at best\nlinear in general"
  },
  {
    "objectID": "lectures/optimization/index.html#levenberg-marquardt",
    "href": "lectures/optimization/index.html#levenberg-marquardt",
    "title": "Optimization",
    "section": "Levenberg-Marquardt",
    "text": "Levenberg-Marquardt\n\nLeast-square minimization: \\(min_x \\sum_i f(x)_i^2 \\in R\\)\nreplace \\({J(x_n)^{\\prime}J(x_n)}^{-1}\\) by \\({J(x_n)^{\\prime}J(x_n)}^{-1} +\\mu I\\)\n\nadjust \\(\\lambda\\) depending on progress\n\nuses only gradient information like Gauss-Newton\nequivalent to Gauss-Newton close to the solution (\\(\\mu\\) small)\nequivalent to Gradient far from solution (\\(\\mu\\) high)"
  },
  {
    "objectID": "lectures/optimization/index.html#consumption-optimization",
    "href": "lectures/optimization/index.html#consumption-optimization",
    "title": "Optimization",
    "section": "Consumption optimization",
    "text": "Consumption optimization\nConsider the optimization problem: \\[\\max U(x_1, x_2)\\]\nunder the constraint \\(p_1 x_1 + p_2 x_2 \\leq B\\)\nwhere \\(U(.)\\), \\(p_1\\), \\(p_2\\) and \\(B\\) are given.\nHow do you find a solution by hand?"
  },
  {
    "objectID": "lectures/optimization/index.html#consumption-optimization-1",
    "href": "lectures/optimization/index.html#consumption-optimization-1",
    "title": "Optimization",
    "section": "Consumption optimization (1)",
    "text": "Consumption optimization (1)\n\nCompute by hand\nEasy:\n\nsince the budget constraint must be binding, get rid of it by stating \\(x_2 = B - p_1 x_1\\)\nthen maximize in \\(x_1\\), \\(U(x_1, B - p_1 x_1)\\) using the first order conditions.\n\nIt works but:\n\nbreaks symmetry between the two goods\nwhat if there are other constraints: \\(x_1\\geq \\underline{x}\\)?\nwhat if constraints are not binding?\nis there a better way to solve this problem?"
  },
  {
    "objectID": "lectures/optimization/index.html#consumption-optimization-2",
    "href": "lectures/optimization/index.html#consumption-optimization-2",
    "title": "Optimization",
    "section": "Consumption optimization (2)",
    "text": "Consumption optimization (2)\n\nAnother method, which keeps the symmetry. Constraint is binding, trying to minimize along the budget line yields an implicit relation between \\(d x_1\\) and \\(d x_2\\) \\[p_1 d {x_1} + p_2 d {x_2} = 0\\]\nAt the optimal: \\(U^{\\prime}_{x_1}(x_1, x_2)d {x_1} + U^{\\prime}_{x_2}(x_1, x_2)d {x_2} = 0\\)\nEliminate \\(d {x_1}\\) and \\(d {x_2}\\) to get one condition which characterizes optimal choices for all possible budgets. Combine with the budget constraint to get a second condition."
  },
  {
    "objectID": "lectures/optimization/index.html#penalty-function",
    "href": "lectures/optimization/index.html#penalty-function",
    "title": "Optimization",
    "section": "Penalty function",
    "text": "Penalty function\n\nTake a penalty function \\(p(x)\\) such that \\(p(x)=K&gt;0\\) if \\(x&gt;0\\) and \\(p(x)=0\\) if \\(x \\leq 0\\). Maximize: \\(V(x_1, x_2) = U(x_1, x_2) - p( p_1 x_1 + p_2 x_2 - B)\\)\nClearly, \\(\\min U \\iff \\min V\\)\nProblem: \\(\\nabla V\\) is always equal to \\(\\nabla U\\).\nSolution: use a smooth solution function like \\(p(x) = x^2\\)\nProblem: distorts optimization\n\nSolution: adjust weight of barrier and minimize \\(U(x_1, x_2) - \\kappa p(x)\\)\n\nPossible but hard to choose the weights/constraints."
  },
  {
    "objectID": "lectures/optimization/index.html#penalty-function-1",
    "href": "lectures/optimization/index.html#penalty-function-1",
    "title": "Optimization",
    "section": "Penalty function",
    "text": "Penalty function\n\nAnother idea: is there a canonical way to choose \\(\\lambda\\) such that at the minimum it is equivalent to minimize the original problem under constraint or to minimize \\[V(x_1, x_2) = U(x_1, x_2) - \\lambda (p_1 x_1 + p_2 x_2 - B)\\]\nClearly, when the constraint is not binding we must have \\(\\lambda=0\\). What should be the value of \\(\\lambda\\) when the constraint is binding ?"
  },
  {
    "objectID": "lectures/optimization/index.html#karush-kuhn-tucker-conditions",
    "href": "lectures/optimization/index.html#karush-kuhn-tucker-conditions",
    "title": "Optimization",
    "section": "Karush-Kuhn-Tucker conditions",
    "text": "Karush-Kuhn-Tucker conditions\n\nIf \\((x^{\\star},y^{\\star})\\) is optimal there exists \\(\\lambda\\) such that:\n\n\\((x^{\\star},y^{\\star})\\) maximizes lagrangian \\(\\mathcal{L} = U(x_1, x_2) + \\lambda (B- p_1 x_1 - p_2 x_2)\\)\n\\(\\lambda \\geq 0\\)\n\\(B- p_1 x_1 - p_2 x_2 \\geq 0\\)\n\\(\\lambda  (B - p_1 x_1 - p_2 x_2 ) = 0\\)\n\nThe three latest conditions are called “complementarity” or “slackness” conditions\n\nthey are equivalent to \\(\\min(\\lambda, B - p_1 x_1 - p_2 x_2)=0\\)\nwe denote \\(\\lambda \\geq 0 \\perp B- p_1 x_1 + p_2 x_2  \\geq 0\\)\n\n\\(\\lambda\\) can be interpreted as the welfare gain of relaxing the constraint."
  },
  {
    "objectID": "lectures/optimization/index.html#karush-kuhn-tucker-conditions-1",
    "href": "lectures/optimization/index.html#karush-kuhn-tucker-conditions-1",
    "title": "Optimization",
    "section": "Karush-Kuhn-Tucker conditions",
    "text": "Karush-Kuhn-Tucker conditions\n\nWe can get first order conditions that factor in the constraints:\n\n\\(U^{\\prime}_x - \\lambda p_1 = 0\\)\n\\(U^{\\prime}_y - \\lambda p_2 = 0\\)\n\\(\\lambda \\geq 0 \\perp B-p_1 x_1 -p_2 x_2 \\geq 0\\)\n\nIt is now a nonlinear system of equations with complementarities (NCP)\n\nthere are specific solution methods to deal with it"
  },
  {
    "objectID": "lectures/optimization/index.html#solution-strategies-for-ncp-problems",
    "href": "lectures/optimization/index.html#solution-strategies-for-ncp-problems",
    "title": "Optimization",
    "section": "Solution strategies for NCP problems",
    "text": "Solution strategies for NCP problems\n\nGeneral formulation for vector-valued functions \\[f(x)\\geq 0 \\perp g(x)\\geq 0\\] means \\[\\forall i, f_i(x)\\geq 0 \\perp g_i(x)\\geq 0\\]\n\nNCP do not necessarily arise from a single optimization problem\n\nThere are robust (commercial) solvers for NCP problems (PATH, Knitro) for that\nHow do we solve it numerically?\n\nassume constraint is binding then non-binding then check which one is good\n\nOK if not too many constraints\n\nreformulate it as a smooth problem\napproximate the system by a series of linear complementarities problems (LCP)"
  },
  {
    "objectID": "lectures/optimization/index.html#smooth-method",
    "href": "lectures/optimization/index.html#smooth-method",
    "title": "Optimization",
    "section": "Smooth method",
    "text": "Smooth method\n\nConsider the Fisher-Burmeister function \\[\\phi(a,b) = a+b-\\sqrt{a^2+b^2}\\]\nIt is infinitely differentiable, except at \\((0,0)\\)\nShow that \\(\\phi(a,b) = 0 \\iff \\min(a,b)=0 \\iff a\\geq 0 \\perp b \\geq 0\\)\nAfter substitution in the original system one can use regular non-linear solver\n\nfun fact: the formulation with a \\(\\min\\) is nonsmooth but also works quite often"
  },
  {
    "objectID": "lectures/optimization/index.html#optimization-libraries",
    "href": "lectures/optimization/index.html#optimization-libraries",
    "title": "Optimization",
    "section": "Optimization libraries",
    "text": "Optimization libraries\n\nRobust optimization code is contained in the following libraries:\n\nRoots.jl: one-dimensional root finding\nNLSolve.jl: multidimensional root finding (+complementarities)\nOptim.jl: minimization\n\nThe two latter libraries have a somewhat peculiar API, but it’s worth absorbing it.\n\nin particular they provide non-allocating algorithms for functions that modify arguments in place\nthey are compatible with automatic differentiation\n\n\njulia&gt; f(x) = [x[1] - x[2] - 1, x[1] + x[2]]\nf (generic function with 1 method)\n\njulia&gt; NLsolve.nlsolve(f, [0., 0.0])\nResults of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [0.0, 0.0]\n * Zero: [0.5000000000009869, -0.5000000000009869]\n * Inf-norm of residuals: 0.000000       \n * Iterations: 1                       \n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true                           \n * Function Calls (f): 2\n * Jacobian Calls (df/dx): 2"
  },
  {
    "objectID": "mie37/index_2026.html",
    "href": "mie37/index_2026.html",
    "title": "econobits",
    "section": "",
    "text": "Lectures\n\n\n\n\n\n\n\n\nConvergence of Sequences (1)\n\n\n\n\n\n\nNo matching items\n\nCommunication\n\nZulip (best)\nEmail to pwinant@escp.eu starting with [mie37]\nGithub issues (PR also welcome)\n\nCoursework\n\ntutorials (optional)\nprojects x2 (mandatory) (50%)\nfinal exam (50%)"
  }
]